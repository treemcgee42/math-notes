\documentclass[12pt]{article}

\usepackage{../preamble}
\usepackage{../linalg}
\usepackage{../tikz_tm42}

\setcounter{secnumdepth}{5}
\newtheorem{para}[theorem]{}

\title{Shifrin MV}
\author{Runi Malladi}

\begin{document}
\maketitle

\section{the derivative} % {{{1 

\subsection{partial, directional derivatives} % {{{2 

\begin{para}
	The goal is to understand a function by knowing how it varies with each variable.
\end{para}

\begin{definition}
	Consider a function $f:U\subset\mathbb{R}^n\to\mathbb{R}^m$, where $U\subset\mathbb{R}^n$ is open. Let $a\in U$. For $j=1,\dots, n$, define
	\begin{equation*}
		\frac{\partial f}{\partial x_j}(a) = D_jf(a) = \lim_{t\to 0}\frac{f(a+te_j)-f(a)}{t}
	\end{equation*}
	provided this limit exists.
\end{definition}

\begin{para}
	The partial derivatives are measuring rate of change along the coordinate axes $e_j$. We can generalize the above definition to an arbitrary direction (vector!) $v$:
\end{para}

\begin{definition}
	Let $f:U\subset \mathbb{R}^n\to\mathbb{R}^m$, where $U\subset\mathbb{R}^n$ is open. Let $a\in U$. For a nonzero $v\in\mathbb{R}^n$, define the \emph{directional derivative} of $f$ at $a$ in the direction of $v$ to be 
	\begin{equation*}
		D_vf(a) = \lim_{t\to 0}\frac{f(a+tv)-f(a)}{t}
	\end{equation*}
	provided this limit exists.
\end{definition}

\begin{remark}
	$D_j f(a) = \frac{\partial f}{\partial x_j}(a) = D_{e_j}f(a)$.
\end{remark}

\begin{para}
	The directional derivative depends not only on the direction of $v$, but also the magnitude of $v$:
\end{para}

\begin{proposition}
	$D_{cv}f(a) = cD_vf(a)$.
\end{proposition}
\begin{proof}
	\begin{align*}
		D_{cv}f(a) =& \lim_{t\to 0} \frac{f(a+tv) - f(a)}{t} = c\lim_{t\to 0}\frac{f(a+tv) - f(a)}{ct} \\
		=& c\lim_{s\to 0}\frac{f(a+sv) - f(a)}{s} = c D_vf(a).
	\end{align*}
\end{proof}

\begin{remark}
	So $D_vf(a)$ is not merely the instantaneous rate of change by an observer at $a$ in the direction of $v$, but perhaps the instantaneous rate of change of an observer at $a$ moving with velocity $v$ (not only the direction of the observer matters, but also their speed).
\end{remark}

\begin{remark}
	In cases where $f:U\subset\mathbb{R}^n \to \mathbb{R}$ consists of functions we can easily differentiate using single-variable calculus rules, the following substitution can help in computing the $D_vf(a)$:
	\begin{gather*}
		\phi: \mathbb{R} \to \mathbb{R} \\
		t \mapsto f(a+tv).
	\end{gather*}
	Then 
	\begin{equation*}
		D_vf(a) = \lim_{t\to 0}\frac{f(a+tv)-f(a)}{t} = \lim_{t\to 0}\frac{\phi(t)-\phi(0)}{t} = \phi'(0).
	\end{equation*}
\end{remark}

% partial, directional derivatives }}}2

\subsection{differentiability} % {{{2 

\begin{example}
	For a function $f:U\subset\mathbb{R}^n\to\mathbb{R}^m$, the existence of all partial derivatives, even all directional derivatives, does not imply continuity.

	Consider the function 
	\begin{gather*}
		f:\mathbb{R}^2 \to \mathbb{R} \\
		(x, y) \mapsto \frac{xy^2}{x^2+y^4}.
	\end{gather*}

	\todo{finish}
\end{example}

\begin{para}
	Recall that the derivative of $f:\mathbb{R}\to\mathbb{R}$ is the best (affine) linear approximation to the graph of $f$ at $a$: for $f'(a)=m$, 
	\begin{equation*}
		\lim_{h\to 0}\frac{f(a+h)-f(a)-mh}{h} = 0.
	\end{equation*}
	This motivates the following:
\end{para}

\begin{definition}
	For $f:U\subset\mathbb{R}^n\to\mathbb{R}^m$, call $f$ \emph{differentiable} at $a\in U$ if there exists a linear map 
	\begin{equation*}
		Df(a): \mathbb{R}^n \to \mathbb{R}^m
	\end{equation*}
	such that 
	\begin{equation*}
		\lim_{h\to 0} \frac{f(a+h)-f(a)-Df(a)h}{\|h\|} = 0.
	\end{equation*}
\end{definition}

\begin{definition}
	Writing $x=a+h$ in the above, define 
	\begin{equation*}
		g(x) = f(a) + Df(a)(x-a)
	\end{equation*}
	to be the \emph{tangent plane} of the graph of $f$ at $a$. It is the best affine linear approximation to $f$ near $a$: by definition of $Df(a)$, 
	\begin{equation*}
		0 = \lim_{h\to 0}\frac{f(a+h)-f(a)-Df(a)h}{\|h\|}=\lim_{x\to a}\frac{f(x)-g(x)}{\|x-a\|}.
	\end{equation*}
\end{definition}

\begin{remark}
	The tangent plane is obtained by translating the graph $\Gamma(Df(a))\subset\mathbb{R}^n\times\mathbb{R}^m$ so that it passes through $(a, f(a))$.
\end{remark}

\begin{proposition}
	If $Df(a)$ exists, it is unique.
\end{proposition}
\begin{proof}
	Suppose there exists linear maps $T,T':\mathbb{R}^n\to\mathbb{R}^m$ such that 
	\begin{equation*}
		\lim_{h\to 0}\frac{f(a+h)-f(a)-Th}{\|h\|} = \lim_{h\to 0}\frac{f(a+h)-f(a)-T'h}{\|h\|}=0.
	\end{equation*}
	Then subtracting gives 
	\begin{equation*}
		\lim_{h\to 0}\frac{(T'-T)(h)}{\|h\|} = 0.
	\end{equation*}
	Let $h=te_i$ for any $i=1,\dots, n$. Then the above implies 
	\begin{equation*}
		\lim_{t\to 0^+}\frac{(T'-T)(te_i)}{t} = (T'-T)(e_i).
	\end{equation*}
	So $Te_i=T'e_i$ for all $i$, implying $T=T'$.
\end{proof}

\begin{proposition}
	If $f$ is vector-valued, it is differentiable at $a$ if and only if each component $f_i$ is differentiable at $a$. Moreover, if say $f=(f_1,\dots, f_n)^T$, then $Df(a)=(Df_1(a),\dots, Df_n(a))^T$.
\end{proposition}
\begin{proof}
	\todo{prove}
\end{proof}

\begin{para}
	As suggseted, differentiability is a stronger condition than the existence of all partial derivatives, and all directional derivatives.
\end{para}

\begin{proposition}
	If $f:U\subset\mathbb{R}^n\to\mathbb{R}^m$ is differentiable at $a$, then the partials $\frac{\partial f_i}{\partial x_j}$ exist. Furthermore, 
	\begin{equation*}
		[Df(a)] = \left[\frac{\partial f_i}{\partial x_j}(a)\right] = [D_jf_i(a)].
	\end{equation*}
	This matrix is called the \emph{Jacobian} of $f$.
\end{proposition}
\begin{proof}
	Since $f$ is differentiable at $a$, there exists a linear map $Df(a)$ such that 
	\begin{equation*}
		\lim_{h\to 0}\frac{f(a+h)-f(a)-Df(a)h}{\|h\|} = 0.
	\end{equation*}
	For any $j=1,\dots, n$, consider $h=te_j$ as $t\to 0$. Then 
	\begin{equation*}
		\lim_{t\to 0}\frac{f(a+te_j)-f(a)-Df(a)(te_j)}{|t|} = 0.
	\end{equation*}
	If $t>0$, by linearity we get 
	\begin{gather*}
		0=\lim_{t\to 0^+} \frac{f(a+te_j)-f(a)-Df(a)(te_j)}{t} = \lim_{t\to 0^+}\frac{f(a+te_j)-f(a)}{t}-Df(a)(e_j), \\
		\lim_{t\to 0^+}\frac{f(a+te_j)-f(a)}{t} = Df(a)e_j.
	\end{gather*}
	Similarly, if $t<0$, 
	\begin{gather*}
		0=\lim_{t\to 0^-} \frac{f(a+te_j)-f(a)-df(a)(te_j)}{-t} = -\left( \lim_{t\to 0^-}\frac{f(a+te_j)-f(a)}{t}-df(a)(e_j) \right), \\
		\lim_{t\to 0^-}\frac{f(a+te_j)-f(a)}{t} = Df(a)e_j.
	\end{gather*}
	thus
	\begin{equation*}
		Df(a)e_j = \lim_{t\to 0}\frac{f(a+te_j)-f(a)}{t} = \frac{\partial f}{\partial x_j}(a).
	\end{equation*}
	So the $j$th column of $[Df(a)]$ is the row vector $[D_jf(a)]_j$. But also, for a fixed $j$, we have $D_jf(a)$ is equal to the column vector $[D_jf_i(a)]_i$. So the $(i,j)$ entry of $[Df(a)]$ is $D_jf_i(a)$ as desired.
\end{proof}

\begin{proposition}
	If $f:U:\mathbb{R}^n\to\mathbb{R}^m$ is differentiable at $a\in U$, then $f$ is continuous at $a$.
\end{proposition}
\begin{proof}
	Suppose $f$ is differentiable at $a$. We want to show $\lim_{x\to a}f(x)=f(a)$, i.e. that $\lim_{h\to 0}f(a+h)=f(a)$. Well
	\begin{equation*}
		\lim_{h\to 0}\frac{f(a+h)-f(a)-Df(a)h}{\|h\|}=0,
	\end{equation*}
	so 
	\begin{equation*}
		\lim_{h\to 0}f(a+h)-f(a)-Df(a)h=\lim_{h\to 0}\frac{f(a+h)-f(a)-Df(a)h}{\|h\|}\|h\|= 0.
	\end{equation*}
	But also $\lim_{h\to 0} Df(a)h = 0$, since $Df(a)$ is a linear map and hence continuous, hence the result.
\end{proof}

\begin{example}
	The following is an example of a function whose partials exist at a point but no other directional derivatives exist at that point and it is not even continuous there:
	\begin{gather*}
		f: \mathbb{R}^2 \to \mathbb{R} \\
		(x,y)^T \mapsto \frac{xy}{x^2+y^2} \\
		(0, 0)^T \mapsto 0
	\end{gather*}
	at $(0, 0)^T$.
\end{example}

\begin{example}
	The following is an example of a function whose partials exist at a point, who is continuous at that point, but not differentiable there:
	\begin{gather*}
		f: \mathbb{R}^2 \to \mathbb{R} \\
		(x, y)^T \mapsto \frac{x^2y}{x^2+y^2} \\
		(0,0)^T \mapsto 0
	\end{gather*}
	at $(0, 0)^T$.
\end{example}

\begin{proposition}
	Let $f$ be differentiable at $a$. Then for all $v\in\mathbb{R}^n$, 
	\begin{equation*}
		D_vf(a) = Df(a)v.
	\end{equation*}
\end{proposition}
\begin{proof}
	We know 
	\begin{equation*}
		\lim_{h\to 0}\frac{f(a+h)-f(a)-Df(a)h}{\|h\|}=0.
	\end{equation*}
	Letting $h=tv$ and $t\to 0$, we get 
	\begin{equation*}
		0 = \lim_{t\to 0}\frac{f(a+tv)-f(a)-Df(a)(tv)}{|t|}
	\end{equation*}
	(we can pull out and discard the constant $\frac{1}{\|v\|}$). By linearity, as before, 
	\begin{align*}
		Df(a)v =& \lim_{t\to 0^+}\frac{f(a+tv)-f(a)}{t} = \lim_{t\to 0^-}\frac{f(a+tv)-f(a)}{t} \\
		=& \lim_{t\to 0}\frac{f(a+tv)-f(a)}{t} = D_vf(a).
	\end{align*}
\end{proof}

\begin{definition}
	A function $f:U\subset\mathbb{R}^n\to\mathbb{R}^m$ with partials which are continuous on $U$ is called \emph{$C^1$}.
\end{definition}

\begin{theorem}
	If $f:U\subset\mathbb{R}^n\to\mathbb{R}^m$ is $C^1$, then $f$ is differentiable.
\end{theorem}
\begin{proof}
	The idea is the following. Fundamentally, continuity of the partials allows us to apply the mean value theorem to the partials. First recall that $f$ is differentiable if and only if each component $f_i$ is differentiable. Now if $f$ is $C^1$ then so it each $f_i$, so it suffices to prove the following simplified claim: if $f:U\subset\mathbb{R}^n\to\mathbb{R}$ is $C^1$, then $f$ is differentiable. 

	We know that if the derivative exists at $a\in U$, then it must be $B=[D_jf(a)]_{j}$. So what we really want to show is that 
	\begin{equation*}
		\lim_{h\to 0}\frac{f(a+h)-f(a) - Bh}{\|h\|} = 0.
	\end{equation*}
	Letting $h=\sum_{j=1}^n h_je_j$, we see $Bh=\sum_{j=1}^n D_jf(a)h_j$. If we can show 
	\begin{equation*}
		\lim_{h\to 0}f(a+h) - f(a) = \sum_{j=1}^n D_jf(a)h_j
	\end{equation*}
	then we would be done.

	The sum on the right is ``partitioning'' the difference on the left dimension-wise. Consider the elements
	\begin{equation*}
		\left\{p_j = a + \sum_{k=1}^j h_ke_k \right\}_{j=0}^n,
	\end{equation*}
	where $p_0=a$. Then 
	\begin{equation*}
		\sum_{j=1}^m f(p_j) - f(p_{j-1}) = f(a+h) - f(a)
	\end{equation*}
	provided $f$ is defined on these points; to ensure this, consider a closed cube $C\subset U\subset \mathbb{R}^n$ centered at $a$ with radius $\epsilon$. Since $f$ is $C^1$, we have that $f$ is defined for all points in $C$ and its partials are continuous at all points in $C$. Now the limit in question takes $h\to 0$, so we may restrict our focus to $h$ such that $\|h\|<\epsilon$. For each such $h$, let $C_h\subset C$ be the cube of radius $\|h\|$ centered at $a$. Then each $p_j$ lies on the boundary of $C_h$, and the above sum makes sense. 

	Fixing $j$, consider the function 
	\begin{equation*}
		\phi(t) = f(p_{j-1}+te_j) = f(q_j)
	\end{equation*}
	where $t\in [0, h_j]$. We see that $p_{j-1}+te_j$ ranges in a line over $[p_{j-1}, p_j]$, and this line is in (on the boundary of) $C_h$. So $\phi(t)$ is well-defined and in fact differentiable: the line is aligned with the coordinate axis $e_j$, so that $\phi(t):\mathbb{R} \to \mathbb{R}$ has derivative $\phi'(t)=D_jf(p_{j-1}+te_j)$. 

	There are two cases. If $h_j=0$, then 
	\begin{equation*}
		f(p_j) - f(p_{j-1}) = f(p_{j-1})-f(p_{j-1})=0 = D_j(q_j)h_j
	\end{equation*}
	where, say, $q_j=a$. If $h_j\neq 0$, then $\phi(t)$ is continuous (it is differnetiable) on $[0, h_j]$ and differentiable on $(0, h_j)$ so by the mean value theorem there exists $c_j\in (0, h_j)$ such that $\phi(h_j)-\phi(0)=\phi'(c_j)h_j$. Letting $q_j=p_{j-1}+c_je_j$, this is equivalent to 
	\begin{equation*}
		f(p_j)-f(p_{j-1}) = D_j(q_j)h_j.
	\end{equation*}
	In both cases, $c_j\in C_h$. We can now write 
	\begin{equation*}
		f(a+h)-f(a) = \sum_{j=1}^m D_jf(q_j)h_j.
	\end{equation*}
	As $h\to 0$, the radius of $C_h$ goes to 0 as well, so that $q_j\to a$. Thus 
	\begin{equation*}
		\lim_{h\to 0}f(a+h)-f(a) = \sum_{j=1}^m D_jf(a)h_j,
	\end{equation*}
	where we have used the continuity of $D_jf$ on $C_h$.
\end{proof}

\begin{example}
	The following is a function which is differentiable but not $C^1$:
	\begin{gather*}
		f: \mathbb{R} \to \mathbb{R} \\
		x \mapsto \begin{cases} x^2\sin(\frac{1}{x}), & x\neq 0 \\ 0, & x=0 \end{cases}
	\end{gather*}
	Then $f'(0)=0$ but $f'(x)$ is not continuous at $0$.
\end{example}

% differentiability }}}2

\subsection{differentiation rules} % {{{2 

\begin{proposition}
	Let $U\subset\mathbb{R}^n$ be open. Let $f,g:U\to\mathbb{R}^m$ and $k:U\to\mathbb{R}$. Suppose $f,g,k$ are differnetiable at $a\in U$. Then, for any $v\in\mathbb{R}^n$,
	\begin{enumerate}
		\item $D(f+g)(a) = Df(a)+Dg(a)$.
		\item $D(kf)(a)v = (Dk(a)v)f(a) + k(a)Df(a)v$.
		\item $D(fg)(a)v = (Df(a)v)g(a) + f(a)(Dg(a)v)$.
	\end{enumerate}
\end{proposition}
\begin{proof}
	We plug in the candidate and check that it satisfies the definition.

	(2) We calculate
	\begin{align*}
		\lim_{h\to 0}& \frac{(kf)(a+h)-(kf)(a)- ((Dk(a)h)f(a)+k(a)(Df(a)h))}{\|h\|} \\
		=& \lim_{h\to 0}\frac{(k(a+h)-k(a))f(a+h) + k(a)(f(a+h)-f(a))}{\|h\|} \\ \quad& -\lim_{h\to 0} \frac{((Dk(a)h)f(a)+k(a)(Df(a)h))}{\|h\|} \\
		=& \lim_{h\to 0}\frac{(k(a+h)-k(a))f(a+h)-(Dk(a)h)f(a)}{\|h\|} \\ \quad& + k(a)\lim_{h\to 0}\frac{f(a+h)-f(a)-Df(a)h}{\|h\|}
	\end{align*}
	where the second term is 0 by definition of the derivative, and 
	\begin{align*}
		\lim_{h\to 0}& \frac{(k(a+h)-k(a))f(a+h)-(Dk(a)h)f(a)}{\|h\|} \\
		=& f(a+h)\lim_{h\to 0}\frac{(k(a+h)-k(a)-(Dk(a)h))}{\|h\|} + \lim_{h\to 0}\frac{(Dk(a)h)(f(a+h)-f(a))}{\|h\|}
	\end{align*}
	where the first term is 0, and 
	\begin{equation*}
		\lim_{h\to 0}\frac{(Dk(a)h)(f(a+h)-f(a))}{\|h\|} = \lim_{h\to 0}\left( Dk(a)\frac{h}{\|h\|} \right)(f(a+h)-f(a)).
	\end{equation*}
	Now 
	\begin{align*}
		0 \leq& \lim_{h\to 0}\left\| Dk(a)\frac{h}{\|h\|} \right\|\cdot \|f(a+h)-f(a)\| \\
		\leq& \lim_{h\to 0}\|Dk(a)\|\cdot \|f(a+h) - f(a)\| \\
		=& 0
	\end{align*}
	where we have used the boundedness of the linear operator $Dk(a)$.
\end{proof}

\begin{corollary}
	Differntion is a linear operator.
\end{corollary}

\begin{theorem}[chain rule]
	Suppose 
	\begin{equation*}
		\mathbb{R}^n \overset{g}{\to} \mathbb{R}^m \overset{f}{\longrightarrow} \mathbb{R}^\ell
	\end{equation*}
	and $g$ is differentiable at $a$ and $f$ is differentiable at $g(a)$. Then $f\circ g$ is differentiable at $a$ and 
	\begin{equation*}
		D(f\circ g)(a) = Df(g(a))Dg(a).
	\end{equation*}
\end{theorem}
\begin{proof}
	We want to show 
	\begin{equation*}
	\label{pf_chain_rule_to_show}
		\lim_{h\to 0}\frac{f(g(a+h))-f(g(a))-(Df(g(a))Dg(a))h}{\|h\|} = 0.\tag{$\ast$}
	\end{equation*}
	Letting $b=g(a)$, we know 
	\begin{gather*}
		\lim_{h\to 0}\frac{g(a+h)-g(a)-Dg(a)h}{\|h\|} = 0 \\
		\lim_{k\to 0}\frac{f(b+k)-f(b)-Df(b)k}{\|k\|} = 0.
	\end{gather*}
	Given $\epsilon>0$, there exists $\delta_1,\eta>0$ such that 
	\begin{gather*}
	\label{pf_chain_rule_hk_ineq}
		\|h\|<\delta_1 \quad \Rightarrow \quad \|g(a+h)-g(a)-Dg(a)h\|<\epsilon\|h\|,\tag{$\ast\ast$} \\
		\|k\|<\eta \quad \Rightarrow \quad \|f(b+k)-f(b)-Df(b)k\| < \epsilon\|k\|.
	\end{gather*}
	Setting $k=g(a+h)-g(a)$,
	\begin{equation*}
		\|h\|<\delta_1 \quad \Rightarrow \quad \|k-Dg(a)h\|<\epsilon\|h\|.
	\end{equation*}
	By the reverse triangle inequality,
	\begin{equation*}
		\|k\|-\|Dg(a)h\| \leq \big| \|k\|-\|Dg(a)h\| \big| \leq \|k-Dg(a)h\|
	\end{equation*}
	so 
	\begin{equation*}
	\label{pf_chain_rule_k_ineq}
		\|k\| < \|Dg(a)h\| + \epsilon\|h\| < (\|Dg(a)\| + \epsilon)\|h\|,\tag{$\ast\ast\ast$}
	\end{equation*}
	where we have used the fact that $\|Dg(a)h\|\leq \|Dg(a)\|\cdot \|h\|$. Now let 
	\begin{equation*}
		\delta_2 = \frac{\eta}{\|Dg(a)\|+\epsilon}, \quad \delta=\min(\delta_1,\delta_2).
	\end{equation*}
	The numerator of (\ref{pf_chain_rule_to_show}) is 
	\begin{align*}
		f(b+k)-f(b)-& (Df(b)Dg(a))h \\
		=& (f(b+k)-f(b)-Df(b)k) + (Df(b)k-Df(b)Dg(a)h) \\
		=& (f(b+k)-f(b)-Df(b)k) + Df(b)(k-Dg(a)h).
	\end{align*}
	Considering $0<\|h\|<\delta$, we get 
	\begin{align*}
		\|f(b+k)-f(b)& -Df(b)Dg(a)h\| \\
		\leq& \|f(b+k)-f(b)-Df(b)k\| + \|Df(b)\|\cdot \|k-Dg(a)k\| \\
		\leq& \epsilon\|k\| + \|Df(b)\|\cdot \epsilon\|h\|
	\end{align*}
	where we have used (\ref{pf_chain_rule_hk_ineq}) and the definitions of $k$ and $Dg(a)$. Continuing,
	\begin{align*}
		\|f(b+k)-f(b)& -Df(b)Dg(a)h\| \\
		\leq& \epsilon\|k\| + \|Df(b)\|\cdot \epsilon\|h\| \\
		<& \epsilon\|h\|\left(\frac{\|k\|}{\|h\|}+\|Df(b)\|\right).
	\end{align*}
	Now 
	\begin{equation*}
		\|k\| \leq \epsilon\|h\| + \|Dg(a)h\| \leq \epsilon\|h\|+\|Dg(a)\|\cdot \|h\|
	\end{equation*}
	by (\ref{pf_chain_rule_k_ineq}), so 
	\begin{align*}
		\|f(b+k)-f(b)& -Df(b)Dg(a)h\| \\
		<& \epsilon\|h\|\left(\frac{\|k\|}{\|h\|}+\|Df(b)\|\right) \\
		\leq& \epsilon\|h\|\big( \|Dg(a)\|+\epsilon+\|Df(b)\|\big).
	\end{align*}
	So whenever $0<\|h\|<\delta$, we have 
	\begin{equation*}
		\frac{\|f(g(a+h))-f(g(a))-Df(g(a))Dg(a)h\|}{\|h\|} < \epsilon(\|Dg(a)\| + \epsilon + \|Df(b)\|).
	\end{equation*}
	Since $\epsilon$ is arbitrary, the limit goes to 0 as $h\to 0$.
\end{proof}

\begin{remark}
	Suppose $f:\mathbb{R}^n\to\mathbb{R}^m$ is differentiable at $a$, and we want to evaluation $D_vf(a)$ for some $v\in\mathbb{R}^n$. 

	Define $g:\mathbb{R}\to\mathbb{R}^n$ by $g(t)=a+tv$, and consider $\phi(t)=(f\circ g)(t)$. Then by definition $D_vf(a)=\phi'(0)$. By the chain rule,
	\begin{equation*}
		D_vf(a)=\phi'(0)=(f\circ g)'(0)=Df(a)g'(0)=Df(a)v
	\end{equation*}
	confirming our earlier result.
\end{remark}

% differentiation rules }}}2

\subsection{gradient} % {{{2 

\begin{definition}
	Let $f:\mathbb{R}^n\to\mathbb{R}$ be differentiable at $a$. The \emph{gradient} of $f$ at $a$ is the vector 
	\begin{equation*}
		\nabla f(a) = (Df(a))^T = \begin{pmatrix} D_1f(a) \\ \vdots \\ D_nf(a) \end{pmatrix}.
	\end{equation*}
\end{definition}

\begin{para}
	Now the directional derivative is 
	\begin{equation*}
		D_vf(a)=Df(a)v=\nabla f(a)\cdot v.
	\end{equation*}
	In particular, this is saying that $\nabla f(a)$ is orthogonal to $v$ where $D_vf(a)=0$, i.e. $\nabla f(a)$ is orthogonal to the $v$ along which $f$ remains constant (instantaneously).

	If $v$ is a unit vector, the Cauchy-Schwartz inequality implies 
	\begin{equation*}
		D_vf(a) \leq \|\nabla f(a)\|,
	\end{equation*}
	and there is equality if and only if $\nabla f(a)$ is a positive scalar multiple of $v$. This implies the following:
\end{para}

\begin{proposition}
	Suppose $f:\mathbb{R}^n\to\mathbb{R}$ is differentiable at $a$. Then 
	\begin{equation*}
		\|\nabla f(a)\|=\max_{\|v\|=1}D_vf(a),
	\end{equation*}
	i.e. 
	\begin{itemize}
		\item $\nabla f(a)$ points in the direction in which $f$ increases at the greatest rate.
		\item $\|\nabla f(a)\|$ is the greates possible (instantaneous) rate of change.
	\end{itemize}
\end{proposition}

% gradient }}}2

\subsection{curves} % {{{2 

\begin{proposition}
	Suppose $g:(a,b)\to\mathbb{R}^n$ is a differentiable parametric curve with the property that $g$ has constant length (i.e. $\|g(t)\|$ is constant for all $t\in (a,b)$). Then $g(t)\cdot g'(t)=0$ for all $t\in (a,b)$.
\end{proposition}
\begin{proof} 
	We know $g(t)\cdot g(t)=c$ for some $c\in\mathbb{R}$. Then, utilizing the product rule,
	\begin{equation*}
		g'(t)\cdot g(t) + g(t)\cdot g'(t) = 2g(t)\cdot g'(t) = 0.
	\end{equation*}
\end{proof}	

\begin{remark}
	The condition above is that the curve lies on a sphere centered at the origin.
\end{remark}

\begin{definition}
	Suppose $g:[a,b]\to \mathbb{R}^n$ is continuous, except perhaps at finitely many points. Define 
	\begin{equation*}
		\int_a^b g(t)dt = \begin{pmatrix} \int_a^b g_1(t)dt \\ \cdots \\ \int_a^b g_n(t)dt \end{pmatrix}.
	\end{equation*}
\end{definition}

\begin{lemma}
	Suppose $g:[a,b]\to \mathbb{R}^n$ is continuous, except perhaps at finitely many points. Then 
	\begin{equation*}
		\left\|\int_a^b g(t)dt \right\| \leq \int_a^b \|g(t)\|dt.
	\end{equation*}
\end{lemma}
\begin{proof} 
	Let $v=\int_a^b g(t)dt$. If $v=0$ then we are done. Otherwise, first note that $|v\cdot g(t)|\leq\|v\|\cdot\|g(t)\|$ by Cauchy Schwartz. Then 
	\begin{align*}
		\|v\|^2 
		=& v\cdot \int_a^b g(t)dt = \int_a^b v\cdot g(t) dt \\
		\leq& \int_a^b \|v\|\cdot \|g(t)\|dt = \|v\|\int_a^b \|g(t)\|dt.
	\end{align*}
	Since $v\neq 0$, we can divide by $\|v\|$ to get the result.
\end{proof}	

\begin{definition}
	Let $g:[a,b]\to\mathbb{R}^n$ be a continuous parameterized curve. Given a partition 
	\begin{equation*}
		\mathcal{P}=\{a=t_0<t_1<\cdots <t_k =b\}
	\end{equation*}
	of $[a,b]$, let 
	\begin{equation*}
		\ell_\mathcal{P}(g) = \sum_{i=1}^k \|g(t_i)-g(t_{i-1})\|.
	\end{equation*}
	Define the \emph{arclength} of $g$ to be 
	\begin{equation*}
		\ell(g) = \sup_\mathcal{P} \ell_\mathcal{P}(g),
	\end{equation*}
	provided this quantity is finite.
\end{definition}

\begin{proposition}
	Let $g:[a,b]\to\mathbb{R}^n$ be a piecewise-$C^1$ paramaterized curve. Then 
	\begin{equation*}
		\ell(g) = \int_a^b \|g'(t)\|dt.
	\end{equation*}
\end{proposition}
\begin{remark}
	This says the distance a particle travels is the integral of its speed.
\end{remark}
\begin{proof} 
	By the lemma, for any $\mathcal{P}$ we have 
	\begin{align*}
		\ell_\mathcal{P}(g) 
		=& \sum_{i=1}^k \|g(t_i)-g(t_{i-1})\| \\
		=& \sum_{i=1}^k \left\|\int_{t_{i-1}}^{t_i}g'(t)dt\right\| \\
		\leq& \sum_{i=1}^k \int_{t_{i-1}}^{t_i} \|g'(t)\|dt = \int_a^b \|g'(t)\|dt,
	\end{align*}
	where we have use the fundamental theorem of calculus which requires the $g_i$ to be continuous on $[t_i,t_{i-1}]$ and differentiable on $(t_i,t_{i-1})$ which is implied by the piecewise $C^1$ hypothesis, and so 
	\begin{equation*}
		\ell(g) \leq \int_a^b\|g'(t)\|dt.
	\end{equation*}

	Now for $a\leq t\leq b$, define $s(t)$ to be the arclength of the curve $g$ on the interval $[a,t]$. Then, for $h>0$,
	\begin{equation*}
		\frac{\|g(t+h)-g(t)\|}{h} \leq \frac{s(t+h)-s(t)}{h} \leq \frac{1}{h} \int_{t}^{t+h}\|g'(u)\|du;
	\end{equation*}
	where the first inequality utilizes the fact that $s(t+h)-s(t)$ is the arclength of $g$ on $[t,t+h]$, which is at least the shortest possible path length $\|g(t+h)-g(t)\|$. The second inequality additionally uses the previous inequality above. 

	First suppose $t<b$. Taking limits, as $h\to 0^+$ the left is 
	\begin{equation*}
		\lim_{h\to 0^+}\frac{\|g(t+h)-g(t)\|}{h} = \|g'(t)\|.
	\end{equation*}
	The right is 
	\begin{equation*}
		\lim_{h\to 0^+}j\frac{1}{h}\int_t^{t+h}\|g'(u)\|du = \|g'(t)\|,
	\end{equation*}
	which can be deduced, for example, by L'Hopital's rule. Hence 
	\begin{equation*}
		\lim_{h\to 0^+} \frac{s(t+h)-s(t)}{h} = \|g'(t)\|
	\end{equation*}
	for all $t\in [a,b)$. Similarly, for $t\in (a,b]$, we can show 
	\begin{equation*}
		\lim_{h\to 0^-} \frac{s(t+h)-s(t)}{h} = \|g'(t)\|.
	\end{equation*}
	Thus on $t\in (a,b)$ we have 
	\begin{equation*}
		\lim_{h\to 0}\frac{s(t+h)-s(t)}{h}=s'(t)=\|g'(t)\|, \quad s(t)=\int_a^t \|g'(u)\|du.
	\end{equation*}
	But $g$ is piecewise continuous on $[a,b]$, hence so is $s(t)$ (ref). Since limits are unique, it follows 
	\begin{gather*}
		\lim_{t\to a}s(t) = \lim_{t\to a}\int_a^t \|g'(u)\|du = 0, \\
		\lim_{t\to b}s(t) = \lim_{t\to a}\int_a^t \|g'(u)\|du = \int_a^b\|g'(u)\|du.
	\end{gather*}
\end{proof}	

\begin{definition}
	We say a parameterized curve $g(t)$ is \emph{arclength-parameterized} if $\|g'(t)\|=1$ for all $t$.
\end{definition}

\begin{remark}
	If $g$ is arclength-parameterized, then $s'(t)=\|g'(t)\|=1$, so $s(t)=t+c$ for some constant $c$. In this case, we often use $s$ as the parameter. But note this is just a matter of notation; if given an arclength-parameterized curve $g:[a,b]\to\mathbb{R}^n$ you can assume that $s$ varies over the domain $[a,b]$ and not over the domain plus some constant.
\end{remark}

In what follows, let $g$ be arclength-parameterized.

\begin{definition}
	Call $g'(s)$ the \emph{unit tangent vector}, and denote it $T(s)$. If $g$ is twice differentiable, let the \emph{curvature} be $\kappa(s)=\|T'(s)\|$. If $g:[0,L]\to\mathbb{R}^n$ is closed, i.e. $g(0)=g(L)$, then the \emph{total curvature} is $\int_0^L \kappa(s)ds$.

	If $T'(s)\neq 0$, define the \emph{principal normal vector} to be $N(s)=T'(s)/\|T'(s)\|$.
\end{definition}

\begin{remark}
	By (ref), $T(s)\cdot T'(s)=0$.
\end{remark}

\begin{proposition}
	For any convex plane curve, total curvature is $2\pi$.
\end{proposition}

% curves }}}2

\subsection{higher order partial derivatives} % {{{2 

\begin{theorem}
	Let $f:U\subset\mathbb{R}^n \to\mathbb{R}^m$ be $C^2$. Then, for any $i,j$, we have 
	\begin{equation*}
		\frac{\partial^2 f}{\partial x_i\partial x_j} = \frac{\partial^2 f}{\partial x_j \partial x_i}.
	\end{equation*}
\end{theorem}
\begin{proof} 
	It suffices to show the result for $m=1$, since we go component-by-component, and for $n=2$, since we can repeat the argument for any two $i,j$ and fixing all other variables as partials do. Without loss of generality, let $i=1, j=2$. Define 
	\begin{gather*}
		\delta\begin{pmatrix} h \\ k \end{pmatrix} = f\begin{pmatrix}a+h \\ b+k \end{pmatrix} - f\begin{pmatrix}a+h \\ b\end{pmatrix} - f\begin{pmatrix} a\\ b+k \end{pmatrix} + f\begin{pmatrix} a \\ b\end{pmatrix}, \\
		q(s) = f\begin{pmatrix}s \\ b+k\end{pmatrix} - f\begin{pmatrix}s \\ b\end{pmatrix}, \\
		r(t) = f\begin{pmatrix} a+h \\ t\end{pmatrix} - f\begin{pmatrix}a \\ t\end{pmatrix}.
	\end{gather*}
	The picture is as follows:

	By the mean value theorem, there exists $\xi\in (a,a+h)$ and $\eta\in (b,b+k)$ such that 
	\begin{align*}
		\delta\begin{pmatrix}h\\k \end{pmatrix}
		=& q(a+h)-q(a) = hq'(\xi) \\
		=& h\left(\frac{\partial f}{\partial x}\begin{pmatrix}\xi \\ b+k\end{pmatrix} - \frac{\partial f}{\partial x}\begin{pmatrix}\xi \\ b\end{pmatrix}\right) \\
		=& hk \frac{\partial^2f}{\partial y\partial x}\begin{pmatrix}\xi \\ \eta\end{pmatrix}.
	\end{align*}
	Our application of the mean value theorem required $f$ to be $C^1$ and twice differentiable. Likewise, there exists $\tau\in (b,b+k)$ and $\sigma\in (a,a+h)$ such that 
	\begin{equation*}
		\delta\begin{pmatrix}h\\ k\end{pmatrix} = hk\frac{\partial^2f}{\partial x\partial y}\begin{pmatrix}\sigma \\ \tau\end{pmatrix}.
	\end{equation*}
	As $h,k\to 0$ we have $\zeta,\sigma\to a$ and $\eta,\tau\to b$. By the continuity of the second partials (using the fact that $f$ is $C^2$), we get the desired equality.
\end{proof}	

% higher order partial derivatives }}}2

% The Derivative }}}1

\section{implict/explicit solutions} % {{{1 

\begin{para} 
	Any subspace $V\subset\mathbb{R}^n$ can be represented in two ways: 
	\begin{itemize}
		\item (explicit) as the span of a collection of vectors.
		\item (implicit) as the space of solutions to an equation $Ax=0$.
	\end{itemize}
	Let's look at the second in more detail. Note $x$ is a solution of $Ax=0$ if and only if $A_{i,\bullet}\cdot x=0$ for all $i$. The equation $A_{i,\bullet}\cdot x=0$ defines a hyperplane in $\mathbb{R}^n$ with normal $(A_{i,\bullet})^T$. Hence the solution space to $Ax=0$ is the intersection of the hyperplanes $A_i\cdot x=0$.
\end{para}	

\subsection{Gaussian elimination} % {{{2 

\begin{definition}
	We call the first nonzero entroy of a row (reading left to right) its \emph{leading entry}.
\end{definition}

\begin{definition}
	A matrix is in \emph{echelon form} if
	\begin{itemize}
		\item the leading entries move right in successive rows.
		\item the entries in the column of each leading entry, below the leading entry, are all $0$.
		\item all rows of zeros are at the bottom of the matrix.
	\end{itemize}
	It is in \emph{reduced echelon form} if it is in echelon form and, additionally, 
	\begin{itemize}
		\item every leading entry is $1$.
		\item all entries of the column above each leading entry are 0 (as well as below).
	\end{itemize}
\end{definition}

\begin{definition}
	A leading entry is called a \emph{pivot} if there is no leading entry above it in the same column. Columns where (a single) pivot appears are called \emph{pivot columns}, and the corresponding variable in the system of equations is called a \emph{pivot variable}. A variable which is not a pivot variable is called a \emph{free variable}.
\end{definition}

\begin{remark}
	When a matrix is in reduced echelon form, we can determine the general solution by expressing each of the pivot variables in terms of the free variables.
\end{remark}

\begin{theorem}
	Suppose $A$ and $B$ are echelon forms of the same nonzero matrix $M$. Then all of their pivots appear in the same positions. Therefore, if they are in reduced echelon form, then they are equal, i.e. the reduced echelon form of the matrix $M$ is unique.
\end{theorem}

\begin{para} 
	Consider $Ax=b$, where $A$ is $m\times n$. Then a solution $c=(c_1\cdots c_n)^T$ has the property that $Ac=b$. As discussed, this means $A_{i,\bullet}\cdot c=b$ for all $i$. Combining this into aa single equation we can also write 
	\begin{equation*}
		b=c_1A_{\bullet, 1}+\cdots +c_nA_{\bullet, n}.
	\end{equation*}
	Thus a solution to $Ax=b$ gives a representation of $b$ as a linear combination of column vectors of $A$.
\end{para}	

\begin{example}
	Suppose we want to express $b$ as a linear combination of vectors $v_1,v_2,v_3$, where 
	\begin{equation*}
		b=\begin{pmatrix}4\\ 3\\ 1\\ 2\end{pmatrix},\quad v_1=\begin{pmatrix}1\\ 0\\ 1\\ 2\end{pmatrix},\quad v_2=\begin{pmatrix}1\\ 1\\ 1\\ 1\end{pmatrix},\quad v_3=\begin{pmatrix}2\\ 1\\ 1\\ 2\end{pmatrix}.
	\end{equation*}
	We want to see if there are solutions $(x_1,x_2,x_3)$ to the equation 
	\begin{equation*}
		x_1v_1+x_2v_2+x_3v_3=b.
	\end{equation*}
	This is the same as solving $Ax=b$ where 
	\begin{equation*}
		A = \begin{pmatrix} 1&1&2 \\ 0&1&1 \\ 1&1&1 \\ 2&1&2 \end{pmatrix}.
	\end{equation*}
\end{example}

\begin{definition}
	Consider a system $Ax=b$. There are two possibilities:
	\begin{itemize}
		\item (\emph{inconsistent}) no solutions. 
		\item (\emph{consistent}) solutions exist.
	\end{itemize}
\end{definition}

\begin{corollary}
	A system in echelon form is inconsistent if and only if there is a row of the form 
	\begin{equation*}
		\begin{bmatrix} 0 & \cdots & 0 & | & c \end{bmatrix}
	\end{equation*}
	for $c\neq 0$. Similarly, a system in echelon form is consistent if and only if any zero row has $c=0$.
\end{corollary}

\begin{corollary}
	If $\text{rank}(A)=m$ then $Ax=b$ is consistent for all $b\in\mathbb{R}^m$.
\end{corollary}

\begin{remark}
	Corresponding to our previous viewpoint:
	\begin{itemize}
		\item (implicit) $Ax=b$ is consistent when the intersection of hyperplanes $A_{i,\bullet}\cdot x=b_i$ is nonempty.
		\item (explicit) $Ax=b$ is consistent when $b$ is in the span of the column vectors $A_{\bullet,j}$.
	\end{itemize}
\end{remark}

\begin{example}
	(constraint equations) We wish to find all $b$ for which $Ax=b$ is consistent. Suppose 
	\begin{equation*}
		A = \begin{pmatrix} 1 & -1 & 1 \\ 3&2&-1 \\ 1&4&-3 \\ 3&-3&3 \end{pmatrix}.
	\end{equation*}
	By the Corollary, we can let $b=(b_1\ b_2\ b_3\ b_4)^T$ can calculate the echelon form of the augmented matrix:
	\begin{equation*}
		[A|b] = \begin{pmatrix} 1 & -1 & 1&b_1 \\ 3&2&-1&b_2 \\ 1&4&-3&b_3 \\ 3&-3&3&b_4 \end{pmatrix} \quad \to \quad \begin{pmatrix} 1 & -1 & 1&b_1 \\ 0&5&-4&b_2-3b_1 \\ 0&0&0&b_3-b_2+2b_1 \\ 0&0&0&b_4-3b_1 \end{pmatrix}.
	\end{equation*}
	So $b$ must satisfy as many constraints as there are rows of zeros in the echelon form (recall by theorem this number is unique).
\end{example}

% Gaussian elimination }}}2

\subsection{existence and uniqueness of solutions} % {{{2

\begin{definition}
	The rank $r$ of a matrix is the number of nonzero rows (i.e. the number of pivots) in its echelon form. 
\end{definition}

\begin{remark}
	Note $r\leq n$ and $r\leq m$. We get that $r\leq n$ since there cannot be more pivots than columns, and $r\leq m$ since there cannot be more nonzero rows than there are total rows.
\end{remark}

We now turn to how many solutions a given consistent system has.

\begin{definition}
	A system $Ax=b$ is \emph{inhomogeneous} when $b\neq 0$ and \emph{homogeneous} when $b=0$.
\end{definition}

\begin{proposition}
	Suppose $Ax=b$ is consistent. Let $u_1$ be an arbitrary solution. Then all solutions are of the form 
	\begin{equation*}
		u = u_1+v
	\end{equation*}
	for some solution $v$ (which is dependent on $u$) of the associated homogeneous system $Ax=0$.
\end{proposition}
\begin{proof}
	First let's show $u$ is a solution:
	\begin{equation*}
		Au = A(u_1+v) = Au_1+Av = b+0=b.
	\end{equation*}
	Now let's show every solution has this form: we need to show $u-u_1$ is always a solution to $Ax=0$:
	\begin{equation*}
		Av = A(u-u_1) = Au-Au_1 = b-b=0.
	\end{equation*}
\end{proof}

\begin{remark}
	This is saying that when the inhomogeneous system $Ax=b$ is constent, its solutions are obtained by translating the set of solutions of the associated homogeneous equation by a "particular solution" $u$.
\end{remark}

\begin{para} 
	Note a homogeneous system is always consistent, since it has the trivial solution $x=0$. If $\text{rank}(A)=n$, then there are $n$ pivot variables and $n-n=0$ free variables. To summarize: the system $Ax=0$ has
	\begin{itemize}
		\item ($\text{rank}(A)=n$) a unique solution, since there are 0 free variables.
		\item ($\text{rank}(A)<n$) infinite solutions, since there are $>0$ free variables.
		\item ($n>m$) infinite solutions, since $\text{rank}(A)\leq m < n$ and above.
	\end{itemize}
\end{para}	

\begin{corollary}
	Suppose $Ax=b$ is consistent. It has a unique solution if and only if $Ax=0$ has a unique (the trivial) solution, which happens if and only if $\text{rank}(A)=n$.
\end{corollary}

\begin{definition}
	An $n\times n$ matrix of rank $n$ is called \emph{nonsingular}. Otherwise, it is called \emph{singular}.
\end{definition}

Recall that if $\text{rank}(A)=m$ then $Ax=b$ is consistent for all $b\in\mathbb{R}^m$.

\begin{theorem}
	Let $A$ be $n\times n$. The following are equivalent:
	\begin{itemize}
		\item $A$ is nonsingular.
		\item $Ax=0$ has only the trivial solution.
		\item $Ax=b$ has a unique solution for all $b\in\mathbb{R}^n$.
	\end{itemize}
\end{theorem}

% existence and uniqueness }}}2

\subsection{elementary matrices, inverses} % {{{2 

\begin{para} 
	There are two interpretations of matrix multiplication $AB$:
	\begin{itemize}
		\item $(AB)_{\bullet, j} = A(B_{\bullet, j})$, i.e. the $j$th column of $AB$ is $A$ times the $j$th column of $B$.
		\item $(AB)_{i,\bullet}=(A_{i,\bullet})B$, i.e. the $i$th row of $AB$ is the $i$th row of $A$ times $B$.
	\end{itemize}
	In particular, 
	\begin{gather*}
		\begin{bmatrix} \vdots & & \vdots \\ a_1 & \cdots & a_n \\ \vdots & & \vdots \end{bmatrix} \begin{bmatrix} x_1 \\ \vdots \\ x_n \end{bmatrix} = x_1a_1 + \cdots + x_n a_n, \\
		\begin{bmatrix} x_1 & \cdots & x_m \end{bmatrix} \begin{bmatrix} \cdots & A_1 & \cdots \\ & \vdots & \\ \cdots & A_m & \cdots \end{bmatrix} = x_1A_1 + \cdots + x_mA_m.
	\end{gather*}
\end{para}	

\begin{proposition}
	Consider the elementary row operations
	\begin{enumerate}
		\item interchangerows $i$ and $j$
		\item multiply row $i$ by a nonzero scalar $c$
		\item row $j$ plus equals $c$ times row $i$.
	\end{enumerate}
	To apply these to an $m\times n$ matrix $A$, multiply $A$ on the left by $I_m'$, where $I_m'$ is the result of having applied the desired row operations to the identity matrix $I_m$.
\end{proposition}

\begin{corollary}
	Consider the augmented matrix $[A|b]$ with echelon form $[U|c]$. Then there exists a matrix $E$ such that 
	\begin{equation*}
		[EA|Eb] = [U|c],
	\end{equation*}
	and moreover $E$ is a product of elementary matrices.
\end{corollary}

\begin{theorem}
	An $n\times n$ matrix is \emph{nonsingular} if and only if it is invertible.
\end{theorem}
\begin{proof} 
	If $A$ is invertible and $Ax=b$, then $x$ can only be $A^{-1}b$.

	Conversely, suppose $A$ is nonsingular. Then $Ax=c$ has a unique solution for all $c$. In particular, there exists a unique $b_j$ such that $Ab_j=e_j$. Let $B=(b_1 \cdots b_n)$. Then $AB=I_n$, so that $B$ is a solution to the equation $AX=I_n$. Then the reduced echelon form of $[A|I_n]$ is $[I_n|B]$, where we have used the fact that $A$ is nonsingular to determine that its reduced echelon form is $I_n$. By the corollary, there exists a product of elementary matrices $E$ such that $E[A|I_n]=[I_n|B]$. Then $EA=I_n$ and $E=B$, so $BA=I_n$.
\end{proof}	

\begin{corollary}
	If $A,B$ are $n\times n$ matrices such that $BA=I_n$, then $B=A^{-1}$ and $A=B^{-1}$, i.e. for square matrices every left inverse is an inverse.
\end{corollary}
\begin{proof} 
	We first claim the homogeneous equation $Ax=0$ has only the trivial solution. To see this, consider that if $x$ is a solution then $BAx=x=0=B(0)$. Now this implies by the proposition that $A$ is nonsingular, hence invertible by the theorem. Then since $BA=I_n$ and inverses are unique, it must be that $B=A^{-1}$.
\end{proof}	

\begin{corollary}
	Given a square matrix $A$, its inverse, if it exists, is the matrix $E$ such that 
	\begin{equation*}
		E[A|I_n] = [I_n | C] 
	\end{equation*}
	for some matrix $C$. In particular, one can apply Gaussian elimination 
	\begin{equation*}
		[A | I_n] \quad \to \quad [I_n | C] 
	\end{equation*}
	and then $A^{-1}=C$, provided we can actually perform such a reduction.
\end{corollary}

\begin{corollary}
	For 
	\begin{equation*}
		A = \begin{pmatrix} a & b \\ c & d \end{pmatrix},
	\end{equation*}
	$A^{-1}$ exists if and only if $ad-bc\neq 0$. In that case, 
	\begin{equation*}
		A^{-1} = \frac{1}{ad-bc} \begin{pmatrix} b & -b \\ -c & a \end{pmatrix}.
	\end{equation*}
\end{corollary}

\begin{proposition}
	A nonsquare matrix never has both a left and right inverse.
\end{proposition}

% elementary matrices, inverses }}}2

\subsection{linear independence, dimension} % {{{2 

\begin{para} 
	We are interested in whether $v\in\text{span}\{v_1,\dots,v_k\}$. Recall that this is asking where the system $Ax=v$ has a solution, where $A_{\bullet, j}=v_j$. We are also interested in whether this solution is unique. Equivalently, this is asking whether (the linear map represented by) $A$ is injective.
\end{para}	

\begin{definition}
	The $\{v_1,\dots, v_k\}$ are \emph{linearly independent} if 
	\begin{equation*}
		c_1v_1 + \cdots c_k v_k = 0 
	\end{equation*}
	implies $c_1=\cdots =c_k=0$. Otherwise, they are called \emph{linearly dependent}.
\end{definition}

\begin{proposition}
	Let $v_1,\dots, v_k\in\mathbb{R}^n$ and let $V=\text{span}\{v_1,\dots, v_k\}$. Then an arbitrary $v\in V$ has a unique expression as a linear combination of $v_1,\dots, v_k$ if and only if the $v_i$ are linearly independent. 
\end{proposition}
\begin{proof}
	($\Leftarrow$) Suppose $v$ is not uniquely expressed, i.e. 
	\begin{equation*}
		v = c_1v_1 + \cdots c_kv_k = d_1v_1 + \cdots d_kv_k
	\end{equation*}
	where $\{c_i\}\neq \{d_i\}$. Then 
	\begin{equation*}
		0 = (c_1-d_1)v_1 + \cdots + (c_k-d_k)v_k 
	\end{equation*}
	and at least one of the coefficients is nonzero. Thus the $\{v_i\}$ are linearly dependent.

	($\Rightarrow$) Suppose $\{v_k\}$ is not linearly independent. Then there exists a nontrivial linear combination 
	\begin{equation*}
		s_1v_1 + \cdots + s_kv_k = 0.
	\end{equation*}
	For any $v\in V$, we can write 
	\begin{equation*}
		v = c_1v_1 + \cdots c_kv_k.
	\end{equation*}
	But also 
	\begin{equation*}
		v = (c_1+s_1)v_1 + \cdots (c_k+s_k)v_k 
	\end{equation*}
	and there exists $i$ such that $s_i\neq 0$ so $c_i+s_i\neq c_i$. So the representation is not unique.
\end{proof}

\begin{remark}
	Recall that a solution to $Ax=b$ is unique if and only if $Ax=0$ has only the trivial solution. By definition, $v\in V$ if and only if $Ax=v$ is consistent. Then uniqueness follows if and only if $Ax=0$ has the trivial solution, which is equivalent to the columns being linearly independent.
\end{remark}

\begin{proposition}
	Suppose $v_1,\dots, v_k\in\mathbb{R}^n$ are linearly independent, and let $x\in\mathbb{R}^n$. Then $\{v_1,\dots,v_k,x\}$ is linearly independent if and only if $x\not\in\text{span}\{v_1,\dots,v_k\}$.
\end{proposition}
\begin{proof} 
	It suffices to show that $\{v_1,\dots,v_k\}$ is linearly dependent if and only if $x\in\text{span}\{v_1,\dots,v_k\}$. 

	($\Leftarrow$) Suppose $x\in\text{span}\{v_1,\dots,v_k\}$. Then $x=c_1v_1+\cdots + c_kv_k$ for some scalaras $c_1,\dots, c_k$. Then 
	\begin{equation*}
		c_1v_1+\cdots +c_kv_k - x = 0
	\end{equation*}
	which is a nontrivial linearly combination, hence $\{v_1,\dots,v_k,x\}$ is linearly dependent.

	($\Rightarrow$) Suppose $\{v_1,\dots,v_k,x\}$ is linearly dependent. Then there exists a nontrivial collection $\{c_k\}$ such that 
	\begin{equation*}
		c_1v_1 + \cdots c_kv_k + cx = 0.
	\end{equation*}
	Note $c\neq 0$, for otherwise the $v_1,\dots,v_k$ would be linearly dependent which is a contradiction to our assumptions. Thus 
	\begin{equation*}
		x = \frac{-1}{c}(c_1v_1 + \cdots + c_kv_k)
	\end{equation*}
	and $x\in\text{span}\{v_1,\dots,v_k\}$.
\end{proof}	

\begin{definition}
	Let $V\subset\mathbb{R}^n$ be a subspace. The set $\{v_1,\dots,v_k\}$ is a basis for $V$ if 
	\begin{itemize}
		\item $v_1,\dots, v_k$ span $V$, and 
		\item $v_1,\dots, v_k$ are linearly independent.
	\end{itemize}
\end{definition}

\begin{corollary}
	Let $V\subset\mathbb{R}^n$ and $v_1,\dots,v_k\in V$. Then $\{v_i\}$ is a basis for $V$ if and only if every vector in $V$ can be written uniquely as a linear combination of $v_1,\dots,v_k$.
\end{corollary}

\begin{definition}
	When we write 
	\begin{equation*}
		v= c_1v_1 + \cdots + c_kv_k,
	\end{equation*}
	we call the $\{c_i\}$ the \emph{coordinates} of $V$ with respect to the ordered basis $\{v_1,\dots,v_k\}$.
\end{definition}

\begin{corollary}
	Let $A$ be $n\times n$. Then $A$ is nonsingular if and only if its column vectors form a basis for $\mathbb{R}^n$.
\end{corollary}

\begin{theorem}
	Any nonzero subspace $V\subset\mathbb{R}^n$ has a basis.
\end{theorem}

\begin{proposition}
	Let $V\subset\mathbb{R}^n$ be a subspace with a basis $\{v_1,\dots,v_k\}$. Let $w_1,\dots, w_l\in V$. If $l>k$, then $\{w_1,\dots, w_l\}$ is linearly dependent.
\end{proposition}
\begin{proof}
	Each $w_j$ can be written uniquely as
	\begin{equation*}
		w_j=\sum_{i=1}^k a_{ij}v_i.
	\end{equation*}
	Now for any $c\in \mathbb{R}^l$, we have 
	\begin{equation*}
		\sum_{j=1}^l c_jw_j = \sum_{j=1}^l c_j (\sum_{i=1}^k c_{ij}v_i) = \sum_{i=1}^k (\sum_{j=1}^l a_{ij}c_j)v_i = 0.
	\end{equation*}
	Since $l>k$, there exists a nonzero $c$ such that $Ac=0$.
\end{proof}

\begin{theorem}
	Let $V\subset\mathbb{R}^n$ be a subspace. Let $\{v_1,\dots,v_k\}$ and $\{w_1,\dots,w_l\}$ be two bases for $V$. Then $k=l$.
\end{theorem}

\begin{definition}
	The \emph{dimension} of a subspace $V\subset\mathbb{R}^n$ is the number of vectors in any basis for $V$. By convention, $\dim\{0\}=0$.
\end{definition}

\begin{lemma}
	Suppose $V,W$ are subspaces of $\mathbb{R}^n$ with the property that $W\subset V$. If $\dim(V)=\dim(W)$, then $V=W$.
\end{lemma}

\begin{proposition}
	Let $V\subset\mathbb{R}^n$ be a $k$-dimensional subspace. Then any $k$ vectors that span $V$ must be linearly independent, and any $k$ linearly independent vectors must span $V$.
\end{proposition}

% linear independence, dimension }}}2

\subsection{four fundamental subspaces} % {{{2 

An $m\times n$ matrix $A$ represents a linear map $\mathbb{R}^n\to\mathbb{R}^m$. There are two fundamental spaces in the domain:
\begin{enumerate}
	\item ($\mathbf{R}(A)$) the row space
	\item ($\mathbf{N}(A)$) the null space (kernel) 
\end{enumerate}
and two fundamental spaces in the codomain:
\begin{enumerate}
	\item ($\mathbf{C}(A)$) the column space (image)
	\item ($\mathbf{N}(A^T)$) the null space of the transpose.
\end{enumerate}

\begin{definition}
	Given an $m\times n$ matrix $A$, the \emph{column space} is 
	\begin{equation*}
		\mathbf{C}(A) = \text{span}\{A_{\bullet, 1}, \dots, A_{\bullet, n}\} \subset\mathbb{R}^m.
	\end{equation*}
	The \emph{row space} is 
	\begin{equation*}
		\mathbf{R}(A) = \text{span}\{A_{1,\bullet},\dots, A_{m,\bullet}\} \subset\mathbb{R}^n.
	\end{equation*}
\end{definition}

\begin{proposition}
	Let $A$ be an $m\times n$ matrix. Let $b\in\mathbb{R}^m$. Then $b\in\mathbf{C}(A)$ if and only if $Ax=b$ is consistent. Thus the column space is the image.
\end{proposition}

\begin{definition}
	The \emph{null space} of $A$ is the set of solutions to the homogeneous equation $Ax=0$. Thus, $\mathbf{N}(A)$ is the kernel.
\end{definition}

\begin{example}
	(Algorithm for finding a basis for $\mathbf{N}(A)$). Reduce to reduced echelon form and read off the solution, e.g. 
	\begin{equation*}
		A = \begin{pmatrix} 1 & 2 & 1 & -1 \\ 1 & 0 & 1 & 1 \end{pmatrix} \quad \to \quad \begin{pmatrix} 1 & 0 & 1 & 1 \\ 0 & 1 & 0 & -1 \end{pmatrix}.
	\end{equation*}
	Reading off the solution:
	\begin{gather*}
		x_1 = -x_3 - x_4 \\
		x_2 = x_4 \\
		x_3 = x_3 \\
		x_4 = x_4,
	\end{gather*}
	which can be written as 
	\begin{equation*}
		\begin{pmatrix} -1 \\ 0 \\ 1 \\ 0 \end{pmatrix}x_3 + \begin{pmatrix} -1 \\ 1 \\ 0 \\ 1 \end{pmatrix}x_4.
	\end{equation*}
	These two vectors are the basis for the null space.
\end{example}

\begin{proposition}
	$\mathbf{N}(A) = \mathbf{R}(A)^\perp$.
\end{proposition}
\begin{proof} 
	If $x\in\mathbf{N}(A)$ then $A_{i,\bullet}x = 0$ for all $i=1,\dots,m$. So $x$ is orthogonal to any linear combination of the $A_{i,\bullet}$, hence orthogonal to any vector in $\mathbf{R}(A)$, i.e. $x\in\mathbf{R}(A)^\perp$. So $\mathbf{N}(A)\subset\mathbf{R}(A)^\perp$. 

	Conversely, let $x\in\mathbf{R}(A)^\perp$. Then $x$ is orthogonal to any linear combination of $A_{i,\bullet}$. In particular, it is orthogonal to each $A_{i,\bullet}$, hence $Ax=0$ so $x\in\mathbf{N}(A)$. So $\mathbf{R}(A)^\perp\subset\mathbf{N}(A)$.
\end{proof}	

\begin{corollary}
	$\mathbf{N}(A^T)=\mathbf{C}(A)^\perp$.
\end{corollary}

\begin{para} 
	So any linear combination of rows of $A$ (i.e. columns of $A^T$) which give 0 (these are the elements of $\mathbf{N}(A^T)$) correspond to a constraint on $\mathbf{C}(A)$ (in particular they must be orthogonal). 

	Let $A$ be $m\times n$. Recall when finding $b\in\mathbb{R}^m$ for which $Ax=b$ was consistent, we obtained constraints on the components of $b$. These determined vectors in $\mathbb{R}^m$ with the property that taking a linear combination of rows of $A$ according to them we should get 0. For example, consider the reduction 
	\begin{equation*}
		\begin{pmatrix}[cc|l] 1 & 2 & b_1 \\ 1 & 1 & b_2 \\ 0 & 1 & b_3 \\ 1 & 2 & b_4 \end{pmatrix} 
		\quad \to \quad 
		\begin{pmatrix}[cc|l] 1 & 2 & b_1 \\ 0 & 1 & b_1-b_2 \\ 0 & 0 & -b_1+b_2+b_3 \\ 0 & 0 & -b_1+b_4 \end{pmatrix}.
	\end{equation*}
	Look at the last two rows. Expressed as vectors, these are 
	\begin{equation*}
		\begin{pmatrix} -1 & 1 & 1 & 0 \end{pmatrix} \quad \begin{pmatrix} -1 & 0 & 0 & 1 \end{pmatrix}.
	\end{equation*}
	Let $y$ be in the span of these two vectors. Then $A^Ty$ is a linear combination of the columns of $A^T$, i.e. the rows of $A$, hence $A^Ty=0$. So $y\in \mathbf{N}(A^T)$. The converse also holds. The corollary says also that $y\in \mathbf{C}(A)^\perp$, and conversely. So the constraints span $\mathbf{N}(A^T)=\mathbf{C}(A)^\perp$. But are these vectors necessarily linearly independent?
\end{para}	

\begin{theorem}
	Let $A$ be $m\times n$. Let $U$ and $R$ be the echelon and reduced echelon forms, respectively. Let $E$ be the product of elementary matrices such that $EA=U$. Then 
	\begin{enumerate}
		\item To get a basis for $\mathbf{R}(A)$, take the nonzero rows of $U$.
		\item To get a basis for $\mathbf{N}(A)$, get the general solution to $Ax=0$ (e.g. by reading off $Rx=0$), set each free variable to 1 and the remaining free variables to 0. 
		\item To get a basis for $\mathbf{C}(A)$, take the pivot columns of $A$.
		\item To get a basis for $\mathbf{N}(A^T)$, take the (transposes of) the rows of $E$ corresponding to the zero rows of $U$.
	\end{enumerate}
\end{theorem}

\begin{theorem}
	Let $A$ be $m\times n$. 
	\begin{enumerate}
		\item $\dim(\mathbf{R}(A))=\dim(\mathbf{C}(A))=\text{rank}(A)$.
		\item $\dim(\mathbf{N}(A))=n-\text{rank}(A)$.
		\item $\dim(\mathbf{N}(A^T))=m-\text{rank}(A)$.
	\end{enumerate}
\end{theorem}

\begin{corollary}[Rank-Nullity theorem]
	$\text{rank}(A) + \dim{\mathbf{N}(A)} = n$.
\end{corollary}

\begin{proposition}
	Let $V\subset\mathbb{R}^n$ be $k$-dimensional. Then $\dim(V^\perp)=n-k$.
\end{proposition}

\begin{proposition}
	For a subspace $V\subset\mathbb{R}^n$, we have $(V^\perp)^\perp=V$.
\end{proposition}

\begin{theorem}
	Let $A$ be $m\times n$. Then 
	\begin{enumerate}
		\item $\mathbf{R}(A) = \mathbf{N}(A)^\perp$.
		\item $\mathbf{R}(A)^\perp = \mathbf{N}(A)$.
		\item $\mathbf{C}(A) = \mathbf{N}(A^T)^\perp$.
		\item $\mathbf{C}(A)^\perp = \mathbf{N}(A^T)$.
	\end{enumerate}
\end{theorem}

\begin{para} 
	So given a subspace $V=\text{span}\{v_1,\dots,v_k\}$, we can equivalently see $V$ as the solution to a homogeneous equation $Ax=0$. For example, Let the rows of $B$ be $v_1,\dots,v_k$. Then $V=\mathbf{R}(A)=\mathbf{N}(A)^\perp$. Let $w_1,\dots,w_l$ be a basis for $\mathbf{N}(B)$. Then $x\in V$ if and only if 
	\begin{equation*}
		w_1\cdot x = \cdots = w_l\cdot x = 0.
	\end{equation*}
	Let $A$ be the matrix whose rows are $w_1,\dots, w_l$. Then $V$ is the solution space to $Bx=0$.
\end{para}	

\begin{proposition}
	For each $b\in\mathbf{C}(A)$, there exists a unique $x\in\mathbf{R}(A)$ such that $Ax=b$. Moreover, of all the solutions to $Ax=b$, it is the one of least length.
\end{proposition}

% four fundamental subspaces }}}2

\subsection{nonlinear case: manifolds} % {{{2 

\begin{para} 
	Recall that given a linear subspace $V$, we can represent it either
	\begin{itemize}
		\item (explicitly) as the span of its basis vectors.
		\item (implicitly) as the solution space of a homogeneous system.
	\end{itemize}
	What about the nonlinear case? Sometimes we can do the same.
\end{para}	

\begin{example}
	For the hyperbola $xy=1$, which is given to us as an implicit representation, we can explicitly solve for $x$ and $y$ and get all solutions, e.g. $x=\frac{1}{y}$ is a ``global'' solution.
\end{example}

\begin{example}
	For the circle $x^2+y^2=1$, another implicit representation, solving for $x$ or $y$ explicitly does not yield all solutions; $y=\sqrt{1-x^2}$ only gives half the possible solutions for $y$, and $y=-\sqrt{1-x^2}$ gives the other half. But for any point on the graph other than $(0,0)$, one of these will give a ``local'' solution, i.e. will be a solution for all points on the graph in a neighborhood of the point.
\end{example}

\begin{para} 
	Our goal is to determine when a coordinate $x_n$ can be expressed implicitly as a function of the others. For example, consider the hyperplane $a\cdot x=0$ in $\mathbb{R}^n$. If $x_n$ (or any $x_k$) is nonzero, then we can write 
	\begin{equation*}
		x_n = \frac{-1}{a_n}(a_1x_1 + \cdots + a_{n-1}x_{n-1}).
	\end{equation*}
	That is, we've expressed the hyperplane as a graph over the $x_1\cdots x_{n-1}$ plane.
\end{para}	

\begin{para} 
	Now if a function is differentiable at a point, then the tangent plane at that point gives the best affine linear approximation of the function near that point. So we might expect that if the tangent plane can be expressed as a graph over the $x_1\cdots x_{n-1}$ plane, then so too can $f$ itself near that point.
\end{para}	

\begin{theorem}[implicit function theorem, simple case]
	Let $f:U\subset\mathbb{R}^n\to\mathbb{R}$ is $C^1$, where $U$ is open. Let $a\in U$, and suppose 
	\begin{equation*}
		f(a) = 0,\quad \frac{\partial f}{\partial x_n}(a)\neq 0.
	\end{equation*}
	Then there exist neighborhoods $V\ni (a_1,\dots,a_{n-1})^T$ and $W\ni a_n$, along with a $C^1$ function 
	\begin{gather*}
		\phi: V \to W \\
		\begin{pmatrix} x_1 \\ \vdots \\ x_{n-1} \end{pmatrix} \mapsto x_n 
	\end{gather*}
	such that, for all $v\in V$, we have 
	\begin{equation*}
		f\begin{pmatrix} v \\ \phi(v) \end{pmatrix} = 0.
	\end{equation*}
	That is, near $a$, the level surface $f=0$ can be expressed as a graph over the $x_1\cdots x_{n-1}$ plane. Thus $f=0$ defines $x_n$ implicitly as a function of the remaining variables.
\end{theorem}

\begin{example}
	Consider the curve 
	\begin{equation*}
		f\begin{pmatrix} x \\ y \end{pmatrix} = y^3-3y-x = 0,
	\end{equation*}
	depicted below (Figure \ref{fig_three_sect_curve}).

	\begin{figure}[h]
	\centering
	\begin{tikzpicture}
	\begin{axis}[
		view={0}{90},
		axis lines=middle,
		axis line style={-},
		xticklabels={},
		yticklabels={},
		xtick={-3,-2,...,3},
	    ytick={-2,-1,...,2},
		xmin=-3.5, xmax=3.5,
        ymin=-2.5, ymax=2.5,
	]
		\addplot3[
			contour gnuplot={levels={0}, labels=false, draw color=funkyfuture8_darkpurple},
			domain y=-2.5:-1
		]
		{y^3-3*y-x};

		\addplot3[
			contour gnuplot={levels={0}, labels=false, draw color=funkyfuture8_lightblue},
			domain y=-1:1
		]
		{y^3-3*y-x};

		\addplot3[
			contour gnuplot={levels={0}, labels=false, draw color=funkyfuture8_brightorange},
			domain y=1:2.5
		]
		{y^3-3*y-x};
	\end{axis}
	\end{tikzpicture}
	\caption{The curve $y^3-3y-x=0$, split into three segments based on where $D_yf(a)=0$.}
	\label{fig_three_sect_curve}
	\end{figure}

	Note this is (globally) a graph of $x$ as a function of $y$, but not vice versa. Now 
	\begin{equation*}
		\frac{\partial f}{\partial y} = 3(y^2-1),
	\end{equation*}
	which is $0$ when at $\pm (-2,1)^T$. By the implicit function theorem, we know that, away from these points, $y$ is given locally implicitly as a function of $x$. This makes sense if we look at Figure \ref{fig_three_sect_curve}, where indeed cutting the curve at these points yields segments which are valid graphs of $y$ as a function of $x$.
\end{example}

\begin{para} 
	We can know legitimize implicit differentiation. The following is an example of how we would carry this out:
	\begin{gather*}
		x^2+y^2=1, \\
		\frac{d}{dx}(x^2+y^2) = \frac{d}{dx}(1), \\
		2x + 2y\frac{dy}{dx} = 0, \\
		\frac{dy}{dx}=\frac{-x}{y}.
	\end{gather*}
\end{para}	

\begin{lemma}[implicit differentiation]
	Let $f:U\subset\mathbb{R}^n$ be $C^1$, and suppose $D_nf(a)\neq 0$. Let $\bar{x}=(x_1,\dots,x_{n-1})^T$. By the implicit function theorem, $f=0$ defines $x_n$ implicitly as a $C^1$ function $\phi(\bar{x})$ near $a$. Then for $j=1,\dots,n-1$ we have 
	\begin{equation*}
		\frac{\partial \phi}{\partial x_j}(\bar{a}) = -\frac{D_jf(a)}{D_nf(a)}.
	\end{equation*}
\end{lemma}
\begin{proof}
	Define 
	\begin{equation*}
		g: V\to\mathbb{R}^n \\
		\bar{x} \mapsto \begin{pmatrix} \bar{x} \\ \phi(\bar{x}) \end{pmatrix},
	\end{equation*}
	where $V$ is the neighborhood of $\bar{x}$ in the statement of the implicit function theorem. Then $(f\circ g)(\bar{x})=0$ for all $\bar{x}\in V$, once again by the theorem. By the chain rule, $D(f\circ g)(\bar{a})=Df(g(\bar{a}))Dg(\bar{a})=0$, so 
	\begin{equation*}
		\begin{pmatrix} D_1f & \cdots D_nf \end{pmatrix} \cdot \begin{pmatrix} 1 & \cdots & 0 \\ \vdots & & \vdots \\ 0 & \cdots & 1 \\ D_1\phi & \cdots & D_{n-1}\phi \end{pmatrix} = 0,
	\end{equation*}
	where all derivatives of $\phi$ are evaluated at $\bar{a}$ and all derivatives of $f$ are evaluated at $g(\bar{a})=a$. Thus for any $j=1,\dots,n-1$ we have 
	\begin{equation*}
		\frac{\partial f}{\partial x_j}(a) + \frac{\partial f}{\partial x_n}(a)\frac{\partial \phi}{\partial x_j}(\bar{a})=0
	\end{equation*}
	and the result follows.
\end{proof}

\begin{example}
	Let's return to the example of $x^2+y^2=1$. Consider the function $f=x^2+y^2-1$. This is indeed a $C^1$ function, and our curve in question is the level curve $f=0$. We calculate $D_yf(a)=2y$, which is nonzero for all $a$ other than $(1, 0)^T$ and $(-1, 0)^T$. So for all other $a$ on the curve, there exists a $C^1$ function $\phi$ expression $y$ in terms of $x$.

	\begin{figure}[h] 
	\centering
	\begin{tikzpicture}
	\begin{axis}[
		view ={0}{90},
		axis lines=middle,
		axis line style={-},
		xticklabels={}, yticklabels={},
		xmin=-1.5, xmax=1.5,
		ymin=-1.5, ymax=1.5,
		samples=50
	]
		\addplot3[
			contour gnuplot={
				levels={0},
				labels=false,
				draw color=funkyfuture8_lightblue
			},
			domain y=-1:0
		]
		{x^2+y^2-1};
	
		\addplot3[
			contour gnuplot={
				levels={0},
				labels=false,
				draw color=funkyfuture8_brightorange
			},
			domain y=0:1
		]
		{x^2+y^2-1};

		\addplot[mark=*, color=funkyfuture8_darkpurple] coordinates {(-1,0) (1,0)};
	\end{axis}
	\end{tikzpicture}
	\caption{Graph of $x^2+y^2-1=0$ with marked points where $D_yf(a)=0$.}
	\end{figure}

	By the lemma, away from the two points, we get 
	\begin{equation*}
		\frac{\partial\phi}{\partial x}(x_0) = -\frac{D_xf(a)}{D_yf(a)}=-\frac{x}{y}=-\frac{x_0}{y_0}.
	\end{equation*}
	In the previous example we got the above value for $\frac{dy}{dx}$. There is no problem here; $\phi$ expresses $y$ as a function of $x$, which is what we implicitly are differentiating in the expression $\frac{dy}{dx}$.
\end{example}

\begin{proposition}
	Suppose $f:U\subset\mathbb{R}^n\to\mathbb{R}$ is $C^1$, and $Df(a)\neq 0$. Suppose $f(a)=c$. Then the tangent hyperplane at $a$ of the level surface $M=f^{-1}(\{c\})$ is given by 
	\begin{equation*}
		T_aM=\{x\in\mathbb{R}^n : Df(a)(x-a)=0\}.
	\end{equation*}
	So in particular $\nabla f(a)$ is normal to the hyperplane.
\end{proposition}
\begin{proof}
	Since $Df(a)\neq 0$, assume without loss of generality that $D_nf(a)\neq 0$. Applying the implicit function theorem to $f-c$, we know $M$ can be expressed near $a$ as the graph $x_n=\phi(\bar{x})$ for some $C^1$ function $\phi$. Now the tangent plane to the graph $x_n=\phi(\bar{x})$ at $(\bar{a}, \phi(\bar{a}))^T$ is, by definition, 
	\begin{equation*}
		g(\bar{x})=\phi(\bar{a})+D\phi(\bar{x}-\bar{a}).
	\end{equation*}
	Writing $x_n=g(\bar{x})$, we get that $x=(\bar{x}, x_n)^T$ is in the graph of the tangent plane and 
	\begin{align*}
		x_n-a_n
		=& D\phi(\bar{a})(\bar{x}-\bar{a}) = \sum_{j=1}^{n-1}\frac{\partial f}{\partial x_j}(\bar{a})(x_j-a_j) \\
		=& \sum_{j=1}^{n-1}\left(-\frac{D_jf(a)}{D_nf(a)}\right)(x_j-a_j).
	\end{align*}
	So 
	\begin{equation*}
		0 = \sum_{j=1}^{n-1}\frac{\partial f}{\partial x_j}(a)(x_j-a_j) + \frac{\partial f}{\partial x_n}(a)(x_n-a_n) = Df(a)(x-a) = 0.
	\end{equation*}
\end{proof}

\begin{para} 
	This is telling us that $M$ is locally represented as a graph over a coordinate hyperplane, e.g. a $C^1$ function $\mathbb{R}^{n-1}\to M$. It turns out that $M$ is an $(n-1)$-dimensional manifold. More generally, a subset $M\subset\mathbb{R}^n$ is an $(n-m)$-dimensional manifold if each point has a neighborhood that is a $C^1$ grpah over some $(n-m)$-dimensional coordinate plane. 

	At this point, it is not obvious that $M$ satisfies the conditions of a manifold. But consider what it means to ``be a $C^1$ graph''. One can show without too much difficulty that if $f:\mathbb{R}^n\to\mathbb{R}^m$ is $C^r$, then $F:\mathbb{R}^n\to\Gamma(f)$ is a $C^r$-diffeomorphism: clearly $F$ is bijective, and $F$ is the composition 
	\begin{gather*}
		F:\mathbb{R}^n \to \mathbb{R}^n\times\mathbb{R}^n \to \Gamma(f) \\
		x\mapsto (x,x) \mapsto (x,f(x))
	\end{gather*}
	which is $C^r$, being the composition of the diagonal map followed by the product of the identity and a $C^r$ map, and the inverse is 
	\begin{gather*}
		F^{-1}: \Gamma(f) \to \mathbb{R}^n \\
		(x,f(x))\mapsto x
	\end{gather*}
	which is a projection map, hence $C^r$.

	The main point is the level of abstraction in the definition of a manifold, and the motivation for it. It was determined that the essential properties of (level) surfaces are those present in the definition of a manifold, and the things we take as definitions of a manifold are provable theorems about level surfaces.
\end{para}	

% nonlinear case: manifolds }}}2

% Implict/explicit solutions }}}1

\section{extremum problems} % {{{1 

\subsection{compactness, maximum value thoerem} % {{{2 

\begin{example}
	Any level set of a continuous function is closed. (TODO)
\end{example}

\begin{theorem}[maximum value theorem]
	Let $X\subset\mathbb{R}^n$ be compact, nad let $f:X\to\mathbb{R}$ be continuous. Then $f$ takes on its maximum and minimum values.
\end{theorem}
\begin{proof} 
	We first claim $f$ is bounded. Suppose otherwise. Then for all $k\in\mathbb{N}$, there exists $x_k\in X$ such that $f(x_k)>k$. Since $X$ is compact, the sequence $\{x_k\}$ has a convergent subsequence, say $x_{k_j}\to a$. Since $f$ is continuous, $f(x_{k_j})\to f(a)<\infty$, a contradiction. Thus $f$ is bounded above. Analagously, $f$ is bounded below.

	Since $f$ is bounded above, it has a least upper bound $M$. So for all $k\in\mathbb{N}$, there is $x_k\in X$ such that $M-f(x_k)<\frac{1}{k}$. Since $X$ is compact, $\{x_k\}$ has a convergent subsequence, say $\{x_{k_j}\}\to z$. Since $f$ is continuous, $f(z)=\lim_{j\to\infty}f(x_{k_j})=M$, so $f$ achieves its maximum value $M$. Analagously, $f$ achieves a minimum value.
\end{proof}	

\begin{example}
	The norm of a linear operator is continuous, so for a linear map $T:\mathbb{R}^n\to\mathbb{R}^m$,
	\begin{equation*}
		\|T\| = \sup_{\|x\|<1}\|T(x)\| = \max_{\|x\|=1}\|T(x)\|.
	\end{equation*}
\end{example}

\begin{proposition}
	Let $T:\mathbb{R}^n\to\mathbb{R}^m$ be a linear map. Then for all $x\in\mathbb{R}^n$, we have 
	\begin{equation*}
		\|T(x)\| \leq \|T\|\cdot \|x\|.
	\end{equation*}
	For any $c\in\mathbb{R}$,
	\begin{equation*}
		\|cT\|=|c|\cdot\|T\|.
	\end{equation*}
	If $S:\mathbb{R}^n\to\mathbb{R}^m$, we also have 
	\begin{equation*}
		\|S+T\|\leq \|S\| + \|T\|.
	\end{equation*}
\end{proposition}

\begin{example}
	For an $n\times n$ diagonal matrix $A$ with diagonal entries $d_1,\dots,d_n$, we have 
	\begin{equation*}
		\|A\|=\max\{|d_1|,\dots,|d_n|\}.
	\end{equation*}
\end{example}

\begin{theorem}[uniform continuity]
	Let $X\subset\mathbb{R}^n$ be compact, and let $f:X\to\mathbb{R}^n$ be continuous. Then $f$ is uniformly continuous.
\end{theorem}
\begin{proof} 
	Suppose $f$ is not uniformly continuous. Then for some $\epsilon_0>0$, there does not exist a single $\delta>0$ such that $\|x-y\|<\delta$ implies $|f(x)-f(y)|<\epsilon$ for all $x,y\in X$. Then for every $m\in\mathbb{N}$, there exists $x_m,y_m\in X$ with $\|x_m-y_m\|<\frac{1}{m}$ and $|f(x_m)-f(y_m)|\geq\epsilon_0$. Since $X$ is compact, the sequence $\{x_k\}$ has a convergent subsequence $\{x_{k_j}\}$ with $x_{k_j}\to a$. By construction, $\|x_m-y_m\|\to 0$, so also $y_{m_k}\to a$. Since $f$ is continuous at $a$, there exists $\delta_0>0$ such that $\|x-a\|<\delta_0$ implies $|f(x)-f(a)|<\frac{\epsilon_0}{2}$. Now pick $k$ large enough such that $\|x_{m_k}-a\|<\delta_0$ and $\|y_{m_k}-a\|<\delta_0$. Then 
	\begin{equation*}
		|f(x_{m_k})-f(y_{m_k})| \leq |f(x_{m_k})| - |f(a)| + |f(a)-f(y_{m_k})| < \epsilon_0.
	\end{equation*}
	But we assumed $|f(x_m)-f(y_m)|\geq\epsilon_0$.
\end{proof}	

% compactness, maximum value thoerem }}}2

\subsection{max/min problems} % {{{2 

\begin{lemma}
	Suppose $f:X\subset\mathbb{R}^n \to\mathbb{R}$ is defined on some neighborhood of the extremum (i.e. local min/max) $a$, and $f$ is differentiable at $a$. Then $Df(a)=\nabla f(a)^T=0$.
\end{lemma}
\begin{proof}
	We will do the case for $a$ being a local minimum, but the case for local maximum is analagous. Suppose $a$ is a local minimum. Then for any $v\in\mathbb{R}^n$, there exists $\delta>0$ such that $|t|<\delta$ implies $f(a+tv)-f(a)\geq 0$. Then, temporarily allowing limits to take on $\pm\infty$,
	\begin{equation*}
		\lim_{t\to 0^+}\frac{f(a+tv)-f(a)}{t}\geq 0, \quad \lim_{t\to 0^-}\frac{f(a+tv)-f(a)}{t}\leq 0.
	\end{equation*}
	But $f$ is differentiable at $a$, so $D_vf(a)$ exists, so 
	\begin{equation*}
		Df(a)v = D_vf(a) = \lim_{t\to 0}\frac{f(a+tv)-f(a)}{t}=0.
	\end{equation*}
	Since $v$ is arbitrary, we have $Df(a)=0$.
\end{proof}

\begin{remark}
	In particular, all partials are $0$, so the tangent plane is horizontal at that point.
\end{remark}

\begin{definition}
	Suppose $f$ is differentiable at $a$. Call $a$ a \emph{critical point} if $Df(a)=0$. If $a$ is a non-extreumum critical point, then it is a \emph{saddle point}. 

	Equivalently, a saddle point is a critical point $a$ such that, for any neighborhood of $a$ there exists $x$ such that $f(x)>f(a)$ and there exists $x$ such that $f(x)<f(a)$, 
\end{definition}

\begin{example}
	The function 
	\begin{equation*}
		f\begin{pmatrix}x \\ y\end{pmatrix} = x^2-y^2
	\end{equation*}
	has a saddle point at the origin.
\end{example}

% max/min problems }}}2

\subsection{quadratic forms, second derivative test} % {{{2 

\begin{lemma}
	Suppose $g:[0,1]\to \mathbb{R}$ is twice differnetiable. Then 
	\begin{equation*}
		g(1)=g(0)+g'(0)+\frac{1}{2}g''(\xi)
	\end{equation*}
	for some $0<\xi<1$.
\end{lemma}
\begin{proof} 
	Define 
	\begin{equation*}
		P(t) = g(0)+g'(0)t+Ct^2,
	\end{equation*}
	where $C=g(1)-g(0)-g'(0)$. Then $P(1)=g(1)$ and $P(0)=g(0)$ and $P'(0)=g'(0)$.

	Now let $h=g-P$, so that 
	\begin{equation*}
		h(0)=h'(0)=h(1)=0.
	\end{equation*}
	By the mean value theorem applied to $h$, there exists $c\in (0,1)$ such that $h'(c)=0$. By the mean value theorem applied to $h'$, there exists $\xi\in (0,c)$ such that $h''(\xi)=0$. Then 
	\begin{equation*}
		g''(\xi)=h''(\xi)+P''(\xi)=P''(xi)=2C,
	\end{equation*}
	so 
	\begin{equation*}
		g(1)=P(1)=g(0)+g'(0)+\frac{1}{2}g''(\xi).
	\end{equation*}
\end{proof}	

\begin{para} 
	We will see that in the multivariable setting, the derivative behaves like a linear map and the second derivative is like a quadratic form.
\end{para}	

\begin{definition}
	Assume $f:U\subset\mathbb{R}^n\to\mathbb{R}$ is a $C^2$ function in a neighborhood of $a$. Define the (symmetric) matrix 
	\begin{equation*}
		\text{Hess}(f)(a) = \left[ \frac{\partial^2f}{\partial x_i\partial x_j}(a) \right],
	\end{equation*}
	called the \emph{Hessian matrix} of $f$ at $a$. Define the associated quadratic form 
	\begin{align*}
		\mathcal{H}_{f,a}:\mathbb{R}^n\to& \mathbb{R} \\
		h \mapsto& h^T(\text{Hess}(f)(a))h \\
				 & = \sum_{i,j=1}^n \frac{\partial^2f}{\partial x_i\partial x_j}(a)h_ih_j.
	\end{align*}
\end{definition}

\begin{proposition}
	Suppose $f:B_r(a)\to\mathbb{R}$ is $C^2$. Then if $\|h\|<r$,
	\begin{equation*}
		f(a+h)=f(a)+Df(a)h+\frac{1}{2}\mathcal{H}_{f,a+\xi h}(h).
	\end{equation*}
	Consequently,
	\begin{equation*}
		f(a+h) = f(a)+Df(a)h+\frac{1}{2}\mathcal{H}_{f,a}(h) + \epsilon(h)
	\end{equation*}
	where
	\begin{equation*}
		\lim_{h\to 0}\frac{\epsilon(h)}{\|h\|^2}.
	\end{equation*}
\end{proposition}
\begin{proof}
	We will apply the lemma to $g(t)=f(a+th)$. Using the chain rule and Theorem 6.1 of Chapter 3, 
	\begin{align*}
		g'(t) =& Df(a+th)h = \sum_{i=1}^n \frac{\partial f}{\partial x_i}(a+th)h_i, \\
		g''(t) =& \sum_{i=1}^n \left( \sum_{j=1}^n \frac{\partial^2 f}{\partial x_j\partial x_i}(a+th)h_j\right)h_i \\
		=& \sum_{i,j=1}^n \frac{\partial^2 f}{\partial x_j\partial x_i}(a+th)h_ih_j = \mathcal{H}_{f,a+th}(h).
	\end{align*}
	Substituting $g$ for $f$ and taking $t=1$ yields the first result.

	For the second, since $f$ is $C^2$ we have $\text{Hess}(f)$ is continuous, so for any $\epsilon>0$ there exists $\delta>0$ such that $\|v\|<\delta$ implies 
	\begin{equation*}
		\|\text{Hess}(f)(a+v)-\text{Hess}(f)(a)\|<\epsilon.
	\end{equation*}
	By Cauchy-Schwartz, $|h^TAh|\leq \|A\|\cdot |h|^2$, so whenever $\|h\|<\delta$ 
	\begin{align*}
		|\mathcal{H}_{f,a+\xi h}(h)-\mathcal{H}_{f,a}(h)|
		=& |h^T(\text{Hess}(f)(a+\xi h)-\text{Hess}(f)(h))h| \\
		\leq& \|h\|^2 (\|\text{Hess}(f)(a+\xi h)-\text{Hess}(f)(h)\|) \\
		<& \epsilon \|h\|^2.
	\end{align*}
	By definition, 
	\begin{equation*}
		\epsilon(h) = \frac{1}{2}(\mathcal{H}_{f,a+\xi h}(h)-\mathcal{H}_{f,a}(h)),
	\end{equation*}
	so 
	\begin{equation*}
		\frac{|\epsilon(h)|}{\|h\|^2} < \frac{\epsilon}{2}
	\end{equation*}
	whenver $\|h\|<\delta$. Since $\epsilon>0$ is arbitrary, we're done.
\end{proof}

\begin{definition}
	Given a symmetric $n\times n$ matrix $A$, call the map 
	\begin{align*}
		Q: \mathbb{R}^n \to& \mathbb{R} \\
		x \mapsto& x^TAx
	\end{align*}
	the \emph{associated quadratic form} of $A$. 

	If for all $x\neq 0$, 
	\begin{itemize}
		\item (positive definite) $Q(x)>0$
		\item (negative definite) $Q(x)<0$
	\end{itemize}
	If $Q(x)=0$ for some $x\neq 0$ and 
	\begin{itemize}
		\item (positive semidefinite) $Q(x)\geq 0$
		\item (negative semidefinite) $Q(x)\leq 0$
	\end{itemize}
	and 
	\begin{itemize}
		\item (indefinite) $Q(x)>0$ for some $x$ and $Q(x)<0$ for some other $x$
	\end{itemize}
\end{definition}

\begin{theorem}
	Suppose $f:B_r(a)\to\mathbb{R}$ is $C^2$ and $a$ is a critical point. 
	\begin{itemize}
		\item If $\mathcal{H}_{f,a}$ is positive (resp. negative) definite, then $a$ is a local minimum (resp. local maximum).
		\item If $\mathcal{H}_{f,a}$ is indefinite, then $a$ is a saddle point.
		\item If $\mathcal{H}_{f,a}$ is positive or negative semidefinite, then we draw no conclusion.
	\end{itemize}
\end{theorem}
\begin{proof} 
	By Proposition 3.2, given any $\epsilon>0$ there exists $\delta>0$ such that 
	\begin{equation*}
		f(a+h)-f(a) = \frac{1}{2}\mathcal{H}_{f,a}(h) + \epsilon(h)
	\end{equation*}
	where 
	\begin{equation*}
		\frac{|\epsilon(h)|}{\|h\|^2}<\epsilon
	\end{equation*}
	whenever $\|h\|<\delta$.

	First suppose $\mathcal{H}_{f,a}$ is positive definite. Well $\mathcal{H}_{f,a}$ is continuous and defined on all of $\mathbb{R}^n$, in particular on $\|x\|=1$, hence by maximum value theorem there exists $m>0$ such that $\mathcal{H}_{f,a}(x)\geq m$ for all unit vectors $x$. So $\mathcal{H}_{f,a}(h)\geq m\|h\|^2$ for all $h$. Let $\epsilon=\frac{m}{4}$. Then 
	\begin{equation*}
		f(a+h)-f(a) = \frac{1}{2}\mathcal{H}_{f,a}(h)+\epsilon(h) > \frac{1}{4}m\|h\|^2 > 0
	\end{equation*}
	for all $\|h\|<\delta$. So $a$ is a local minimum. The negative definite case is analagous.

	Now suppose $\mathcal{H}_{f,a}$ is indefinite. Then there exists unit $x,y$ such that $\mathcal{H}_{f,a}(x)=m_1>0$ and $\mathcal{H}_{f,a}(y)=m_2<0$. Let $\epsilon=\frac{1}{4}\min(m_1,-m_2)$. Let $h=tx$ (resp. $ty)$ with $|t|<\delta$. Then 
	\begin{equation*}
		f(a+tx)=f(a)>\frac{1}{4}m_1 t^2>0
	\end{equation*}
	and 
	\begin{equation*}
		f(a+ty)-f(a)<\frac{1}{4}m_2 t^2<0.
	\end{equation*}
	So $a$ is a saddle point.
\end{proof}	

\begin{corollary}
	Let $f:U\subset\mathbb{R}^2\to\mathbb{R}$ be $C^2$, and let $a\in U$ be a critical point. Writing 
	\begin{equation*}
		\text{Hess}(f)(a) = \begin{pmatrix} A & B \\ B & C \end{pmatrix} = \begin{pmatrix} D_{x,x}f(a) & D_{x,y}f(a) \\ D_{y,x}f(a) & D_{y,y}f(a) \end{pmatrix},
	\end{equation*}
	then 
	\begin{itemize}
		\item (local min) $AC-B^2>0$ and $A>0$
		\item (local max) $AC-B^2>0$ and $A<0$
		\item (saddle point) $AC-B^2<0$
		\item (inconclusive) $AC-B^2=0$
	\end{itemize}
\end{corollary}

\begin{proposition}
	Suppose $A$ is a symmetric matrix with associated quadratic form $Q$.

	Suppose $A = LDL^T$, where $L$ is lower triangular with $1$'s in the diagonal and $D$ is diagonal.
	\begin{itemize}
		\item If all entries of $D$ are positive (resp. negative), then $Q$ is positive (resp. negative) definite.
		\item If all entries of $D$ are nonnegative (resp, nonpositive) and at least one is $0$, then $Q$ is positive (resp. negative) semidefinite.
		\item If entries of $D$ have opposite, then $Q$ is indefinite.
	\end{itemize}

	Conversely,
	\begin{itemize}
		\item If $Q$ is positive (resp. negative) definite, then there exists a lower triangular matrix $L$ with $1$'s in its diagonal, and a diagonal matrix $D$ with positive (resp. negative) entries, such that $A=LDL^T$.
		\item If $Q$ is semidefinite (resp. indefinite), the matrix $EAE^T$, where $E$ is a suitable product of elementary matrices, can be written as $LDL^T$ where there is at least one $0$ (resp. entries with opposite signs) on the diagonal of $D$.
	\end{itemize}
\end{proposition}

% quadratic forms, second derivative test }}}2

\subsection{Lagrange multipliers} % {{{2 

\begin{definition}
	Suppose 
	\begin{gather*}
		f: U\subset\mathbb{R}^n \to \mathbb{R} \\
		g: U\subset\mathbb{R}^n \to \mathbb{R}^m 
	\end{gather*}
	are differentiable. Suppose $g(a)=0$.

	We say $a$ is a local max/min point of $f$ subject to the constraint $g(x)=0$ if, for some $\delta>0$, we have $f(x)\leq f(a)$ (resp. $f(x)\geq f(a)$) for all $x\in B(a,\delta)$ satisfying $g(x)=0$.

	Succintly, letting $M=g^{-1}(\{0\})$, $a$ is an extremum of $f$ restricted to $M$.
\end{definition}

\begin{theorem}
	Suppose $U\subset\mathbb{R}^n$ is open, $f:U\to\mathbb{R}$ is differentiable and $g:U\to\mathbb{R}^m$ is $C^1$. Suppose $g(a)=0$ and $\text{rank}(Dg(a))=m$.

	If $a$ is a local extremum of $f$ subject to the constraint $g=0$, then there exists $\lambda_1,\dots,\lambda_m$ such that 
	\begin{equation*}
		Df(a) = \lambda_1Dg_1(a) + \cdots + \lambda_m Dg_m(a).
	\end{equation*}
	These scalars are called \emph{Lagrange multipliers}.
\end{theorem}

\begin{remark}
	This condition is necessary, but not sufficient. For instance, there may be constrained saddle points.
\end{remark}

\begin{proof}
	By the implicit function theorem, we can represent $M=g^{-1}(\{0\})$ locally near $a$ as a graph over some coordinate $(n-m)$-plane, let's say locally 
	\begin{equation*}
		M = \left\{ \begin{pmatrix} \bar{x} \\ \phi(\bar{x}) \end{pmatrix} : \bar{x}\in V\subset\mathbb{R}^{n-m}\right\},
	\end{equation*}
	where $\phi:V\to\mathbb{R}^m$ is $C^1$. Define a local parameterization of $M$ by 
	\begin{align*}
		\Phi:V\to& \mathbb{R}^n \\
		\bar{x} \mapsto& \begin{pmatrix} \bar{x} \\ \phi(\bar{x}) \end{pmatrix}.
	\end{align*}
	Observe that 
	\begin{itemize}
		\item $g\circ \Phi=0$
		\item $f\circ\Phi$ has a local extremum at $\bar{a}$, by assumption.
	\end{itemize}
	Differentiating,
	\begin{equation*}
		Dg(a)D\Phi(\bar{a})=0,\quad Df(a)D\Phi(\bar{a})=0. \tag{$\ast$}
	\end{equation*}
	The first equation in ($\ast$) says that $\tau=\text{Im}(D\Phi(\bar{a}))$ satisfies $\tau\subset\ker(Dg(a))$. By the rank-nullity theorem,
	\begin{equation*}
		\text{dim}(\mathbf{N}(Dg(a))) = n - \text{rank}(Dg(a)) = n-m,
	\end{equation*}
	by assumption on $\text{rank}(Dg(a))$. But $\tau$ itself is $(n-m)$-dimensional by assumption on $\phi$, so it must be that $\tau=\mathbf{N}(Dg(a))=(\mathbf{R}(Dg(a)))^\perp$.

	The second equation in ($\ast$) says 
	\begin{equation*}
		\tau\subset\mathbf{N}(Df(a)) = \mathbf{R}(Df(a))^\perp,
	\end{equation*}
	so 
	\begin{equation*}
		\mathbf{R}(Dg(a))^\perp \subset \mathbf{R}(Df(a))^\perp,
	\end{equation*}
	so 
	\begin{equation*}
		\mathbf{R}(Df(a)) \subset \mathbf{R}(Dg(a)),
	\end{equation*}
	so $Df(a)$ is a linear combination of linear maps $Dg_1(a), \dots, Dg_m(a)$, i.e. $\nabla f(a)$ is a linear combination of $\nabla g_1(a),\dots,\nabla g_m(a)$.
\end{proof}

\begin{remark}
	The subspace $\tau=\text{im}(D\Phi(\bar{a}))=\mathbf{R}(Dg(a))^\perp$ is the \emph{tangent space} of $M$ at $a$.
\end{remark}

% Lagrange multipliers }}}2

\subsection{projections, least squares} % {{{2 

\begin{para} 
	Let $A$ be an $(m\times n)$-matrix, let $x$ be an $(n\times 1)$-matrix, let $y$ be an $(m\times 1)$ matrix. Sometimes the equation 
	\begin{equation*}
		Ax=y
	\end{equation*}
	may not have an explicit solution ($y\not\in\mathbf{C}(A)$). Then question then becomes to find a solution $Ax=p$ where $p$ is the vector in $\mathbf{C}(A)$ closest to $b$.
\end{para}	

\begin{definition}
	Let $V\subset\mathbb{R}^n$ be a subspace, let $b\in\mathbb{R}^n$. Define the \emph{projection} of $b$ onto $V$ to be the unique vector $p\in V$ with the property that $b-p\in V^\perp$. We write $p=\text{proj}_v(b)$.
\end{definition}

\begin{proposition}
	Projection is a linear map.
\end{proposition}

\begin{para} 
	Now $V$ can either be given explicity ($V=\mathbf{C}(A)$) or implicitly ($V=\mathbf{N}(B)$).

	Suppose $A$ is a $(m\times n)$-matrix of rank $m$. Say the column vectors $A_{\bullet, 1},\dots,A_{\bullet, m}$ give a basis for $V=\mathbf{C}(A)$. Define 
	\begin{align*}
		f: \mathbb{R}^n \to& \mathbb{R} \\ 
		x \mapsto& \|Ax-b\|^2.
	\end{align*}
	We want to find the critical points of $f$. 

	Write 
	\begin{gather*}
		h(y) = \|y\|^2 = y_1^2 + \cdots + y_m^2, \\
		g(x) = Ax-b
	\end{gather*}
	so that $f=h\circ g$. Then 
	\begin{gather*}
		Dh(y) = 2y_1 + \cdots + 2y_m = 2y^T, \\
		Dg(x) = A.
	\end{gather*}
	So by chain rule,
	\begin{equation*}
		Df(x) = Dh(Ax-b)Dg(x) = 2(Ax-b)^T A, 
	\end{equation*}
	so $Df(x)=0$ if and only if $(Ax-b)^TA=0$, i.e.
	\begin{gather*}
		(Ax-b)^TA=0, \\ 
		A^T(Ax-b) = 0, \\
		A^TAx=A^Tb.
	\end{gather*}
	Now $A^TA$ is a symmetric $(n\times n)$-matrix, hence nonsingular, hence has a unique solution $\bar{x}$. 

	We claim $\bar{x}$ is a global minimum. Well, $A^T(A\bar{x}-b)=0$ implies 
	\begin{equation*}
		A\bar{x}-b\in\mathbf{N}(A^T)=\mathbf{C}(A)^\perp.
	\end{equation*}
	So for any $x\in\mathbb{R}^n$ such that $x\neq\bar{x}$, we have 
	\begin{align*}
		f(x) 
		=& \|Ax-b\|^2 \\
		=& \|A(x-\bar{x}) + (A\bar{x}-b)\|^2 \\
		=& \|A(x-\bar{x})\|^2 + \|A\bar{x}-b\|^2 \\
		>& f(\bar{x}), 
	\end{align*}
	where in the third line we used $A\bar{x}-b\in\mathbf{C}(A)^\perp$ and $A(x-\bar{x})\in\mathbf{C}(A)$.
\end{para}	

\begin{definition}
	We call $\bar{x}$ the \emph{least squares solution}. To recap, it is the solution of least length to \emph{associated normal equation} 
	\begin{equation*}
		A^TAx = A^Tb \tag{$\ast$}
	\end{equation*}
	and the global minimum of $\|Ax-b\|^2$.
\end{definition}

\begin{remark}
	When $A$ has rank $<m$, the linear system ($\ast$) has infinitely many solutions, so we need to pick the least squares solution to be the one of least length.
\end{remark}

\begin{proposition}
	For $V\subset\mathbb{R}^n$, the map
	\begin{equation*}
		\text{proj}_V:\mathbb{R}^n\to\mathbb{R}^n
	\end{equation*}
	is given by 
	\begin{equation*}
		P = A(A^TA)^{-1}A^T,
	\end{equation*}
	where $A_{\bullet, j}=v_j$, where $v_1,\dots,v_m$ is a basis for $V$.
\end{proposition}
\begin{proof} 
	Given $b\in\mathbb{R}^n$, we know that, letting $\bar{x}=(A^TA)^{-1}A^Tb$, that $A\bar{x}=p=\text{proj}_V(b)$.
\end{proof}	

So far, we've been working with $V$ explicitly defined. What about implicitly defined?

Suppose an $m$-dimensional subspace $V\subset\mathbb{R}^n$ is the nullspace of an $(n-m)\times n$ matrix $B$ of rank $n-m$. To find the point in $V$ closest to $b\in\mathbb{R}^n$, we want to minimize 
\begin{align*}
	f:\mathbb{R}^n \to& \mathbb{R} \\
	x \mapsto& \|x-b\|^2
\end{align*}
subject to the constraint $g(x)=Bx=0$.

We can use Lagrange multipliers here.
\begin{gather*}
	Df(x) = \lambda Dg(x), \\
	(x-b)^T = \sum_{i=1}^{n-m}\lambda_i B_{i,\bullet}, \\
	x-b = B^T\lambda.
\end{gather*}
Multiplying by $B$ and using the constraint equation, 
\begin{equation*}
	(BB^T)\lambda = -Bb.
\end{equation*}
By analogy with our treatment of ($\ast$), the matrix $BB^T$ has rank $n-m$, and so we can solve for $\lambda$, hence for the constrained extremum $x_0$:
\begin{align*}
	x_0 
	=& b + B^T\lambda = b + B^T(-(BB^T)^{-1}Bb) \\
	=& b - B^T(BB^T)^{-1}Bb.
\end{align*}
By the projection formula, 
\begin{align*}
	x_0 
	=& b - \text{proj}_{\mathbf{C}(B^T)}b = b-\text{proj}_{\mathbf{R}(B)}b = \text{proj}_{\mathbf{R}(B)^\perp}b \\
	=& \text{proj}_{\mathbf{N}(B)}b.
\end{align*}

% projections, least squares }}}2

\subsection{data fitting} % {{{2 

Consider trying to find the right constants $a$ and $k$ so that the points 
\begin{equation*}
	\begin{pmatrix} x_1 \\ y_1 \end{pmatrix}, \dots, \begin{pmatrix} x_m \\ y_m \end{pmatrix}
\end{equation*}
lie on the curve $y=ax^k$. Taking logs, this is equivalent to fitting the points 
\begin{equation*}
	\begin{pmatrix} u_i \\ v_i \end{pmatrix} = \begin{pmatrix} \log(x_i) \\ \log(y_i) \end{pmatrix}
\end{equation*}
to a line 
\begin{equation*}
	v = \log(ax^k) = \log(a) + k\log(x) = \log(a) + ku.
\end{equation*}
The least squares solution of such problems is called the least squares line, or a line of regression.

In other words, finding the least squares line $y=\bar{a}x+\bar{b}$ is finding the least squares solution of the (inconsistent) system 
\begin{equation*}
	A\begin{pmatrix} a \\ b \end{pmatrix} = y,
\end{equation*}
where 
\begin{equation*}
	A = \begin{pmatrix} x_1 & \cdots & 1 \\ \vdots & & \vdots \\ x_m & \cdots & 1 \end{pmatrix},\quad y=\begin{pmatrix} y_1 \\ \vdots \\ y_m \end{pmatrix}.
\end{equation*}
Let $\bar{y}=A(\bar{a}, \bar{b})^T$ be the projection of $y$ onto $\mathbf{C}(A)$. The least squares solution $(\bar{a}, \bar{b})^T$ has the property that $\|y-\bar{y}\|$ is as small as possible. Define the error vector 
\begin{equation*}
	\epsilon = y - \bar{y}.
\end{equation*}
Then 
\begin{equation*}
	\epsilon = \begin{pmatrix} \epsilon_1 \\ \vdots \\ \epsilon_m \end{pmatrix} = \begin{pmatrix} y_1-\bar{y_1} \\ \vdots \\ y_m-\bar{y_m} \end{pmatrix} = \begin{pmatrix} y_1 - (\bar{a}x_1+\bar{b}) \\ \vdots \\ v_m - (\bar{a}x_m+\bar{b}) \end{pmatrix}.
\end{equation*}
Recall $\epsilon=y-\bar{y}\in\mathbf{C}(A)^\perp$. So $\epsilon$ is orthogonal to each column vector of $A$. So 
\begin{equation*}
	\begin{pmatrix} \epsilon_1 \\ \vdots \\ \epsilon_m \end{pmatrix} \cdot \begin{pmatrix} 1 \\ \vdots \\ 1 \end{pmatrix} = \epsilon_1 + \cdots + \epsilon_m = 0.
\end{equation*}

% data fitting }}}2

\subsection{orthogonal bases} % {{{2 

\begin{lemma}
	Suppose $\{v_1,\dots,v_k\}$ is a basis for $V$, and $x\in V$. Then it is an orthogonal basis if and only if  
	\begin{equation*}
		x = \sum_{i=1}^k \text{proj}_{v_i}x = \sum_{i=1}^k \frac{x\cdot v_i}{\|v_i\|^2}v_i.
	\end{equation*}
\end{lemma}

\begin{proposition}
	Let $V\subset\mathbb{R}^n$ be a $k$-dimensional subspace. For any vector $b\in\mathbb{R}^n$, 
	\begin{equation*}
		\text{proj}_V(b) = \sum_{i=1}^k \text{proj}_{v_i}b = \sum_{i=1}^k \frac{b\cdot v_i}{\|v_i\|^2}v_i \tag{$\ast\ast$}
	\end{equation*}
	if and only if $\{v_1,\dots,v_k\}$ is an orthogonal basis for $V$.
\end{proposition}
\begin{proof} 
	($\Rightarrow$) Assume $\{v_1,\dots,v_k\}$ is an orthogonal basis for $V$. Let $p=\text{proj}_Vb$. Then $b-p\in V^\perp$ and $p\in V$. Since $p\in V$, 
	\begin{equation*}
		p = \sum_{i=1}^k \frac{p\cdot v_i}{\|v_i\|^2}v_i.
	\end{equation*}
	Since $b-p\in V^\perp$, $(b-p)\cdot v_i=0$ so $b\cdot v_i = p\cdot v_i$ for all $i$. Then 
	\begin{align*}
		\text{proj}_Vb 
		=& p = \sum_{i=1}^k \text{proj}_{v_i}p = \sum_{i=1}^k \frac{p\cdot v_i}{\|v_i\|^2}v_i \\
		=& \sum_{i=1}^k \frac{b\cdot v_i}{\|v_i\|^2}v_i = \sum_{i=1}^k \text{proj}_{v_i}b.
	\end{align*}
	($\Leftarrow$) Suppose 
	\begin{equation*}
		\text{proj}_Vb = \sum_{i=1}^k \text{proj}_{v_i}b
	\end{equation*}
	for all $b\in\mathbb{R}^n$. In particular, when $b\in V$, we get $b=\text{proj}_Vb$ so $b$ can be written as a linear combination of $v_1,\dots,v_k$. So these $v_i$ span $V$, have the same dimension as $V$, hecne are a basis. By the lemma, it must be orthogonal.
\end{proof}	

\begin{theorem}[Gram-Schmidt process]
	Given a basis $\{v_1,\dots,v_k\}$ for a subspace $V\subset\mathbb{R}^n$, we obtain an orthogonal basis $\{w_1,\dots,w_k\}$ for $V$ as follows: 
	\begin{align*}
		w_1 =& v_1 \\ 
		w_2 =& v_2 - \frac{v_2\cdot w_1}{\|w_1\|^2}w_1 \\
		\vdots& \\
		w_{j+1} =& v_{j+1} - \frac{v_{j+1}\cdot w_1}{\|w_1\|^2}w_1 - \cdots - \frac{v_{j+1}\cdot w_j}{\|w_j\|^2}w_j \\
		\vdots& \\
		w_k =& v_k - \frac{v_k\cdot w_1}{\|w_1\|^2}w_1 - \cdots - \frac{v_k\cdot w_{k-1}}{\|w_{k-1}\|^2}w_{k-1}.
	\end{align*}
	These can be normalized to an orthonormal basis.
\end{theorem}

\begin{theorem}[Lagrange interpolation formula]
	Given $k+1$ points 
	\begin{equation*}
		\begin{pmatrix} t_1 \\ b_1 \end{pmatrix}, \cdots, \begin{pmatrix} t_{k+1} \\ b_{k+1} \end{pmatrix}
	\end{equation*}
	in the plane with $t_1,\dots, t_{k+1}$, there is exactly one polynomial $p\in \mathcal{P}_k$ whose graph passes through the points.
\end{theorem}
\begin{proof} 
	We will construct an orthonormal basis for $\mathcal{P}_k$. Let 
	\begin{equation*}
		p_i(t) = \frac{(t-t_1)\cdots\widehat{(t-t_i)}\cdots (t-t_{k+1})}{(t_i-t_1)\cdots\widehat{(t_i-t_i)}\cdots (t_i-t_{k+1})}.
	\end{equation*}
	Then 
	\begin{equation*}
		p_i(t_j) = \begin{cases} 1, & i=j \\ 0, & i\neq j \end{cases}
	\end{equation*}
	since $p_i(t_i)=1$ and $p_i(t_j)=0$ for $i\neq j$. Then 
	\begin{equation*}
		p = b_1p_1 + \cdots + b_{k+1}p_{k+1},
	\end{equation*}
	has the desired properties. For uniqueness, any polynomial $q$ such that $q(t_j)=b_j$ for $j=1,\dots,k+1$ must satisfy 
	\begin{equation*}
		\langle q,p_j \rangle = b_j.
	\end{equation*}
\end{proof}	

% orthogonal bases }}}2

% extremum problems }}}1

\section{solving nonlinear problems} % {{{1 

\subsection{contraction mapping principle} % {{{2 

\begin{proposition}
	Suppose $\{a_k\}$ is a sequence of vectors in $\mathbb{R}^n$, and the series 
	\begin{equation*}
		\sum_{k=1}^\infty \|a_k\|
	\end{equation*}
	converges. Then the series 
	\begin{equation*}
		\sum_{k=1}^\infty a_k
	\end{equation*}
	converges.
\end{proposition}

\begin{definition}
	Let $X\subset\mathbb{R}^n$. A function $f:X\to Y$ is called a \emph{contraction mapping} if there is a constant $0<c<1$ such that 
	\begin{equation*}
		\|f(x)-f(y)\| \leq \|x-y\|
	\end{equation*}
	for all $x,y\in\mathbb{X}$. 
\end{definition}

\begin{theorem}[contraction mapping principle]
	Let $X\subset\mathbb{R}^n$ be closed. Let $f:X\to X$ be a contraction mapping. Then there exists a unique fixed point, i.e. a unique $x\in X$ such that $f(x)=x$. 
\end{theorem}
\begin{proof} 
	Let $x_0\in X$ be arbitrary, and define a sequence 
	\begin{equation*}
		x_{k+1}=f(x_k).
	\end{equation*}
	We claim this converges to a point $x\in X$. Suppose for a moment that this is true. Then 
	\begin{equation*}
		f(x) = \lim_{k\to \infty}f(x_k) = \lim_{k\to\infty}x_{k+1}=x.
	\end{equation*}
	This will show the existence of a fixed point (but not uniqueness). To prove the claim, consider 
	\begin{align*}
		x_k 
		=& x_0 + (x_1-x_0) + \cdots + (x_k-x_{k-1}) \\
		=& x_0 + \sum_{j=1}^k (x_j-x_{j-1}).
	\end{align*}
	Set $a_k=x_k-x_{k-1}$. It suffices to show that the sequence of $a_k$ converges, for 
	\begin{equation*}
		\lim_{k\to\infty} = \lim_{k\to\infty} x_0 + \sum_{j=1}^k a_j.
	\end{equation*}
	(Note that since each $x_k\in X$, so is their limit, if it exists). Well by definition of the $\{x_k\}$ and the contraction, 
	\begin{align*}
		\|a_k\| 
		=& \|x_k-x_{k-1}\| = \|f(x_{k-1})-f(x_{k-2})\| \\
		\leq& c\|x_{k-1}-x_{k-2}\| = c\|a_{k-1}\|
	\end{align*}
	for some $0<c<1$, so 
	\begin{align*}
		\|a_k\| 
		\leq& c\|a_{k-1} \leq c^2 \|a_{k-2}\| \leq \cdots \\
		\leq& c^{k-1}\|a_1\|.
	\end{align*}
	Thus 
	\begin{equation*}
		\sum_{k=1}^K \|a_k\| \leq \left(\sum_{k=1}^K c^{k-1}\right)\|a_1\| = \frac{1-c^K}{1-c}\|a_1\|.
	\end{equation*}
	Since $0<c<1$, 
	\begin{equation*}
		\lim_{K\to\infty}\frac{1-c^K}{1-c} = \frac{1}{1-c}
	\end{equation*}
	so $\sum \|a_k\|$ converges. By the proposition, then $\sum a_k$ converges to some $a\in\mathbb{R}^n$. Uniqueness is left as an exercise.
\end{proof}	

\begin{proposition}[mean value inequality]
	Suppose $f:U\subset\mathbb{R}^n\to\mathbb{R}^m$ is $C^1$. Suppose $a,b\in U$ are such that the line segment between them is in $U$. Then 
	\begin{equation*}
		\|f(b)-f(a)\| \leq \left( \text{max}_{x\in [a,b]} \|Df(x)\| \right) \|b-a\|.
	\end{equation*}
\end{proposition}
\begin{proof} 
	Define 
	\begin{align*}
		g:[0,1]\to \mathbb{R}^m \\
		t \mapsto& f(a+t(b-a)).
	\end{align*}
	Note $f(b)-f(a)=g(1)-g(0)$. By the chain rule, $g$ is differentiable and 
	\begin{equation*}
		g'(t) = Df(a+t(b-a))(b-a). \tag{$\ast$}
	\end{equation*}
	By lemma 5.3 ch 3, 
	\begin{align*}
		\|f(b)-f(a)\| 
		=& \left\| \int_0^1 g'(t)dt \right\| \leq \int_0^1 \|g'(t)\|dt \\
		\leq& \text{max}_{t\in [0,1]}\|g'(t)\|.
	\end{align*}
	By ($\ast$), we have $\|g'(t)\leq \|Df(a+t(b-a))\|\cdot \|b-a\|$, so 
	\begin{align*}
		\text{max}_{t\in [0,1]}\|g'(t)\| 
		\leq& \left( \text{max}_{t\in [0,1]} \|Df(a+t(b-a))\| \right) \|b-a\| \\
		=& \left( \text{max}_{x\in [a,b]} \|Df(x)\| \right) \|b-a\|.
	\end{align*}
	Here we have used the operator inequality $\|Tx\|\leq \|T\|\cdot \|x\|$.
\end{proof}	

% contraction mapping principle }}}2

\subsection{inverse, implicit function theorems} % {{{2 

\begin{theorem}[inverse function theorem]
	Suppose $f:U\subset\mathbb{R}^n\to\mathbb{R}^n$ is $C^1$. Let $x_0\in U$, and suppose $Df(x_0)$ is invertible. Then there exists a nehborhood $x_0\in V\subset U$ of on which $f$ has a $C^1$ inverse function. 

	(That is, there exist neighborhood $V\ni x_0$ and $W\ni f(x_0)=y_0$ and a $C^1$ function $g:W\to Y$ such that $f$ is an inverse to $g$.)

	Moreover, for any $x\in V$, write $f(x)=y$. Then $Dg(y)=(Df(x))^{-1}$.
\end{theorem}
\begin{proof} 
	First observe that, without loss of generality, we can assume $x_0=y_0=0$ and $Df(0)=I$. Indeed, $f$ is (locally) invertible if and only if 
	\begin{itemize}
		\item it is invertible after a translation of the domain by $-x_0$.
		\item it is invertible after a translation of the codomain by $-y_0$.
		\item $Df(0)^{-1}f(x)$ is invertible. For if $f$ is invertible, then $(f^{-1}(y)Df(0))(Df(0)^{-1}f(x))=$. On the other hand, if $Df(0)^{-1}f(x)$ is invertible, then 
			\begin{gather*}
				f(x) = Df(0)Df(0)^{-1}f(x), \\
				f(x)(Df(0)^{-1}f(x))^{-1}(Df(0))^{-1} = I, \\
				(Df(0)^{-1}f(x))^{-1} Df(0)^{-1}f(x) = I
			\end{gather*}
			so $f$ is invertible.
	\end{itemize}
	With that simplification in hand, let's continue.

	Since $f$ is $C^1$, there exists $r>0$ such that $\|x\|\leq r$ implies $\|Df(x)-I\|\leq \frac{1}{2}$. Fix $y$ with $\|y\|\leq \frac{r}{2}$. Define 
	\begin{equation*}
		\phi(x) = x - f(x) + y.
	\end{equation*}
	Then $\|D\phi(x)\| = \|Df(x)-I_n\|$. Now when $x\leq r$, we have 
	\begin{equation*}
		\|\phi(x)\| \leq \|x-f(x)\| + \|y\|. \tag{$\ast$}
	\end{equation*}
	Let's examine $\|x-f(x)\|$. Consider the function $(f-I)(x)$. Note $(f-I)(x)=f(x)-x$ and $(f-I)(0)=0$. The mean value inequality says 
	\begin{gather*}
		\|(f-I)(\bar{x}) - (f-I)(0)\| \leq \left( \text{max}_{x\in [a,b]} \|D(f-I)(x)\| \right) \|\bar{x}-0\| \\
		\|x-f(x)\| \leq \text{max}_{x\in [0,\bar{x}]}\|Df(x)-I\|\cdot \|\bar{x}\| \leq \frac{r}{2}.
	\end{gather*}
	Thus (claim?) $\|\phi(x)\leq r$. So $\phi$ maps the closed ball $\overline{B_r(0)}$ to itself. Moreover, if $x,y\in B_r(0)$, by Proposition 1.3, we have 
	\begin{equation*}
		\|\phi(x)-\phi(y)\|\leq\frac{1}{2}\|x-y\|
	\end{equation*}
	so $\phi$ is a contraction mapping on $\overline{B_r(0)}$. By the theorem, it has a unique fixed point $x_y\in\overline{B_r(0)}$, hence such that $f(x_y)=y$. Check $x_y\in B_r(0)$.

	Now let 
	\begin{align*}
		W =& B_{r/2}(0) \\
		V =& f^{-1}(W) \cap B_r(0)
	\end{align*}
	and 
	\begin{align*}
		g: W\to& V \\
		y \mapsto& x_y.
	\end{align*}
	We claim $g$ is continuous. Define 
	\begin{align*}
		\psi: B_r(0) \to& \mathbb{R}^n \\
		x \mapsto& f(x)=x.
	\end{align*}
	Then 
	\begin{equation*}
		\|(f(u)-u) - f(v)) - (u-v)\| = \|\psi(u)-\psi(v)\| \leq \frac{1}{2}\|u-v\|.
	\end{equation*}
	Thus 
	\begin{equation*}
		\|(f(u)-f(v)) - (u-v)\| \leq \frac{1}{2}\|u-v\|.
	\end{equation*}
	By the reverse triangle inequality,
	\begin{equation*}
		\|u-v\|-\|f(u)-f(v)\| \leq \frac{1}{2}\|u-v\|
	\end{equation*}
	so 
	\begin{equation*}
		\frac{1}{2}\|u-v\|\leq \|f(u)-f(v)\|.
	\end{equation*}
	Writing $f(u)=y$ and $f(v)=z$, we have 
	\begin{align*}
		\|g(y)-g(z)\| 
		\leq& 2\|f(g(y))-f(g(z))\| = 2\|f(x_y)-f(x_z)\| \\ 
		\leq& 2\|y-z\| \tag{$\ast\ast$}
	\end{align*}
	hence $g$ is continuous.

	Now we claim $g$ is differentiable. Fix $y\in W$ and write $g(y)=z$. We want to show $Dg(y)=(Df(x))^{-1}$. Choose $k$ small enough that $y+k\in W$. Set $g(y+k)=x+h$, such that $h=g(y+k)-g(y)$. Write $A=Df(x)$. We want to show 
	\begin{equation*}
		\lim_{k\to 0} \frac{g(y+k)-g(y)-A^{-1}k}{\|k\|} = 0.
	\end{equation*}
	Multiply by $A$:
	\begin{equation*}
		\frac{A(g(y+k)-g(y))-k}{\|k\|} = \frac{Ah-k}{\|k\|} = - \frac{f(x+h)-f(x)-Df(x)h}{\|h\|}\cdot \frac{\|h\|}{\|k\|}
	\end{equation*}
	where we have used the fact that 
	\begin{align*}
		f(x+h)-f(x) 
		=& f(g(y+k)) - f(g(y)) \\
		=& y+k-y = k.
	\end{align*}
	Now ($\ast\ast$) implies $\|h\|\leq 2\|k\|$, so $h\to 0$ as $k\to 0$ since $f$ is differentiable. Moreover, $h\neq 0$ when $k\neq 0$ for $h=g(y+k)-g(y)$ and is injective on a neighborhood of those points. So the above product goes to $0$.

	It remains to check $g$ is $C^1$. We have 
	\begin{equation*}
		Dg(y) = (Df(g(y)))^{-1},
	\end{equation*}
	so $Dg$ is the composition of the function $y\mapsto Df(g(y))$ and $A\mapsto A^{-1}$. Since $g$ is continuous and $f$ is $C^1$, the composition must be continuous. Inversions are continuous, hence $y\mapsto Dg(y)$ is continuous.
\end{proof}	

\begin{remark}
	Can generalize this to $C^k$ functions.
\end{remark}

\begin{example}
	($Df$ is everywhere invertible, but $f$ is not injective.) Define 
	\begin{align*}
		f: \mathbb{R}^2 \to& \mathbb{R}^2 \\
		\begin{pmatrix}u \\ v\end{pmatrix} \mapsto& \begin{pmatrix}e^u\cos(v) \\ e^u\sin(v) \end{pmatrix}.
	\end{align*}
	Then $f$ is $C^1$ and 
	\begin{equation*}
		Df\begin{pmatrix} u \\ v\end{pmatrix} = \begin{pmatrix} e^u\cos(v) & -e^u\sin(v) \\ e^u\sin(v) & e^u\cos(v) \end{pmatrix}
	\end{equation*}
	is everywhere nonsingular since its determinant is $e^{2u}\neq 0$. But since sine and cosine are periodic, $f$ is not one-to-one.

	On the other hand, if $f(u,v)^T = (x,y)^T$ then we can write 
	\begin{equation*}
		g\begin{pmatrix} x \\ y\end{pmatrix} = \begin{pmatrix} \frac{1}{2}\log(x^2+y^2) \\ \arctan(y/x) \end{pmatrix}
	\end{equation*}
	and then $f\circ g(x,y)^T = (x,y)^T$. So why is $g$ not the inverse of $f$? Well arctan is a function $\mathbb{R}\to (-\frac{\pi}{2}, \frac{\pi}{2})$.

	Let's calculate the derivative of any local inverse $g$, according to the theorem. If $f(u,v)^T=(x,y)^T$ then 
	\begin{align*}
		Dg\begin{pmatrix}x\\ y\end{pmatrix} 
		=& \left( Df\begin{pmatrix}u\\ v\end{pmatrix}\right)^{-1} = \begin{pmatrix} e^{-u}\cos(v) &  e^{-u}\sin(v) \\ -e^{-u}\sin(v) & e^{-u}\cos(v) \end{pmatrix}^{-1} \\
		=& \begin{pmatrix} \frac{x}{x^2+y^2} & \frac{y}{x^2+y^2} \\ \frac{-y}{x^2+y^2} & \frac{x}{x^2+y^2} \end{pmatrix}.
	\end{align*}
	Indeed, this agrees with the derivative of our specific $g$.
\end{example}

In what follows, let $n>m$.

\begin{theorem}[implicit function theorem]
	Suppose $F:U\subset\mathbb{R}^n\to\mathbb{R}^m$ is $C^1$. Write a vector in $\mathbb{R}^n$ as $(x,y)^T$ where $x\in\mathbb{R}^{n-m}$ and $y\in\mathbb{R}^m$.	

	Suppose $F(x_0,y_0)^T=0$ and the $m\times m$ matrix $\frac{\partial F}{\partial y}(x_0, y_0)^T$ is invertible. Then there exist neighborhoods $V\ni x_0$ and $W\ni y_0$ and a $C^1$ function $\phi:V\to W$ such that 
	\begin{equation*}
		F\begin{pmatrix}x\\ y\end{pmatrix}=0, x\in V, y\in W \quad \Leftrightarrow \quad y=\phi(x).
	\end{equation*}
	Moreover, 
	\begin{equation*}
		D\phi(x) = -\left( \frac{\partial F}{\partial y}\begin{pmatrix}x \\ \phi(x)\end{pmatrix}\right)^{-1} \frac{\partial F}{\partial x}\begin{pmatrix}x \\ \phi(x) \end{pmatrix}.
	\end{equation*}
\end{theorem}
\begin{proof} 
	Define 
	\begin{align*}
		F: U\to& \mathbb{R}^n=\mathbb{R}^{n-m}\times\mathbb{R}^m  \\
		\begin{pmatrix} x \\ y \end{pmatrix} \mapsto& \begin{pmatrix} x \\ F(x,y)^T \end{pmatrix}.
	\end{align*}
	Note the linear map 
	\begin{equation*}
		Df\begin{pmatrix}x_0 \\ y_0\end{pmatrix} = \begin{pmatrix} I & 0 \\ \frac{\partial F}{\partial x}(x_0,y_0)^T & \frac{\partial F}{\partial y}(x_0, y_0)^T \end{pmatrix}
	\end{equation*}
	is invertible (Exercise 4.2.7). (Note the rows are size $n-m$ and $m$, and the columns are size $n-m$ and $m$ as well.) So there exist neighborhoods $V\subset\mathbb{R}^{n-m}$ of $x_0$ and $W\subset\mathbb{R}^m$ of $y_0$ and $Z\subset\mathbb{R}^n$ of $0$ and a $C^1$ function $g:Z\to V\times W$ such that $g$ is an inverse of $f$ on $V\times W$. Now define 
	\begin{align*}
		\phi:V\to& W \\
		\begin{pmatrix} x\\ phi(x) \end{pmatrix} \mapsto& g\begin{pmatrix}x\\ 0\end{pmatrix}.
	\end{align*}
	Since $g$ is $C^1$, so is $\phi$. And 
	\begin{equation*}
		\begin{pmatrix}x \\ F(x,\phi(x))^T \end{pmatrix} = f\begin{pmatrix}x\\ \phi(x)\end{pmatrix} = f(g(\begin{pmatrix}x\\ 0\end{pmatrix})) = \begin{pmatrix}x\\ 0\end{pmatrix},
	\end{equation*}
	so $F(x,\phi(x))^T=0$ as desired. On the other hand, if $F(x,y)^T=0$, $x,V$, $y\in W$, then $(x,y)^T=g(x,0)^T$ so 
	\begin{equation*}
		\begin{pmatrix}x\\ 0\end{pmatrix} = \begin{pmatrix}x\\ F(x,y)^T\end{pmatrix} = f\begin{pmatrix}x\\y\end{pmatrix}=f(g\begin{pmatrix}x\\ 0\end{pmatrix})
	\end{equation*}
	so $y=\phi(x)$, where we have used the fact that $f$ is injective on this neighborhood so $(x,y)^T=g(x,0)^T=(x,\phi(x))^T$. 

	Now that we know $\phi$ is $C^1$, we calculate the derivative using... Define 
	\begin{align*}
		h:V\to& \mathbb{R}^m \\
		x\mapsto& F(x,\phi(x))^T.
	\end{align*}
	Then $h$ is $C^1$. Also $h(x)=0$ for all $x\in V$, so we get 
	\begin{align*}
		0 = Dh(x) 
		=& DF\begin{pmatrix}x\\ \phi(x)\end{pmatrix} D(x\mapsto \begin{pmatrix}x\\\phi(x)\end{pmatrix})(x) \\
		=& \begin{pmatrix} \frac{\partial F}{\partial x}\begin{pmatrix}x\\ \phi(x)\end{pmatrix} & \frac{\partial F}{\partial y}\begin{pmatrix}x\\ \phi(x)\end{pmatrix} \end{pmatrix} \begin{pmatrix} I \\ D\phi(x) \end{pmatrix} \\
		=& \frac{\partial F}{\partial x}\begin{pmatrix}x\\ \phi(x)\end{pmatrix} + \frac{\partial F}{\partial y}\begin{pmatrix}x\\ \phi(x)\end{pmatrix}D\phi(x),
	\end{align*}
	and use the invertibility of $\frac{\partial F}{\partial y}\begin{pmatrix}x\\ \phi(x)\end{pmatrix}$. (Note in above, the sizes are as follows: the first term is $m\times n$, with the columns being divided as $n-m$ and $m$. The second term is $n\times X$ with rows divided as $n-m$ and $m$).
\end{proof}	

% inverse, implicit function theorems }}}2

\section{manifolds, revisited} % {{{1 

\begin{para} 
	We wish to make clear three equivalent formulations of a $k$-dimensional manifold $M$:
	\begin{itemize}
		\item (explicit) $M$ is locally a graph over some $k$-dimensional coordinate plane.
		\item (implicit) $M$ is locally the level set of some function whose derivative has maximum rank.
		\item (parametric) $M$ is locally parameterized by some function whose derivative has maximal rank (e.g. a parametric curve with nonzero velocity).
	\end{itemize}
	More precisely:
\end{para}	

\begin{definition}
	We say $M\subset\mathbb{R}^n$ is a $k$-dimensional manifold if any of the following equivalent criteria hold:
	\begin{enumerate}
		\item for all $p\in M$, there exists a neighborhood $W\subset\mathbb{R}^n$ of $p$ such that $M\cap W$ is the graph of a smooth function $f:V\subset\mathbb{R}^k\to\mathbb{R}^{n-k}$.
		\item for all $p\in M$, there exists a neighborhood $W\subset\mathbb{R}^n$ of $p$ and a smooth function $W\to\mathbb{R}^{n-k}$ such that $F^{-1}(0)=M\cap W$ and $\text{rank}(Df(x))-n-k$ for all $x\in M\cap W$.
		\item for all $p\in M$, there exist a neighborhood $W\subset\mathbb{R}^n$ of $p$ such that $M\cap W$ is the image of a smooth function $g:U\subset\mathbb{R}^k\to\mathbb{R}^n$ which is one-to-one, $\text{rank}(Dg(u))=k$ for all $u\in U$, and $g^{-1}:M\cap W\to U$ is continuous.
	\end{enumerate}
\end{definition}

\begin{theorem}
	The three criteria are equivalent.

	($2\Rightarrow 1$) Implicit function theorem.

	($1\Rightarrow 3$) Set $g(x)=(u, f(u))^T$, WLOG.

	($3\Rightarrow 2$) Suppose $\tilde{W}\subset\mathbb{R}^n$ and $g:U\subset\mathbb{R}^k\to\mathbb{R}^n$ are as in the definition. The fact that $g^{-1}:M\cap\tilde{W}\to U$ is continuous tells us that if $g(u_0)=p$, then points sufficiently close to $p\in M$ must map close to $u_0$ under $g^{-1}$. So all points of $M\cap\tilde{W}$ are the image of a neighborhood of $u_0$ under $g$.

	Renumbering coordinates in $\mathbb{R}^n$ as necessary, we assume $g(0)=p$ and $Dg(0) = (A, B)^T$ where $A$ is an invertible $k\times k$ matrix. Define 
	\begin{align*}
		G:U\times\mathbb{R}^{n-k} \to& \mathbb{R}^n \\
		\begin{pmatrix}u\\ v\end{pmatrix} \mapsto& g(u) + \begin{pmatrix} 0 \\ v\end{pmatrix}.
	\end{align*}
	Then 
	\begin{equation*}
		Dg\begin{pmatrix}0\\ 0\end{pmatrix} = \begin{pmatrix}A & 0\\ B* I_{n-k}\end{pmatrix}
	\end{equation*}
	is invertible. By the inverse function theorem, there exist neighborhoods $V=V_1\times V_2\subset\mathbb{R}^k\times\mathbb{R}^{n-k}$ of $(0,0)^T$ and $W\subset\mathbb{R}^n$ of $p$ and a local (smooth) inverse $H:W\to V$ of $G$. Shrinking $W$ if necessary, we assume $W\subset\tilde{W}$. Write 
	\begin{equation*}
		H(X) = \begin{pmatrix} H_1(x) \\ H_2(x) \end{pmatrix} \in \mathbb{R}^k\times\mathbb{R}^{n-k},
	\end{equation*}
	and define 
	\begin{align*}
		F: W\to& \mathbb{R}^{n-k} \\ 
		x \mapsto H_2(x).
	\end{align*}
	Now suppose $F(x)=0$. Since $x\in W$, $x=G(u,v)^T$ for a unique $(u,v)^T\in V$. Then 
	\begin{equation*}
		F(x) = F(G\begin{pmatrix}u\\ v\end{pmatrix}) = H_2(G\begin{pmatrix}u\\ v\end{pmatrix}) = v,
	\end{equation*}
	so $F(x)=0$ if and only if $v=0$, so $x=g(u)$. So the equation $F=0$ defines that portion of $M$ given by $g(u)$ for all $u\in V$, but $W\subset\tilde{W}$, so such points are all of $M\cap W$.
\end{theorem}

\begin{example}
	Let 
	\begin{align*}
		g:\mathbb{R} \to& \mathbb{R}^3 \\
		u \mapsto& \begin{pmatrix}u\\ u^2\\ u^3\end{pmatrix}
	\end{align*}
	and $M=\text{im}(g)$.

	We want to write $M$ (perhaps locally) as the level set of a function near $p=0$. Define 
	\begin{equation*}
		G\begin{pmatrix}u\\ u^2\\ u^3\end{pmatrix} = \begin{pmatrix}u\\ u^2\\ u^3\end{pmatrix} + \begin{pmatrix}0\\ v_1\\ v_2\end{pmatrix} = \begin{pmatrix}u\\ u^2+v_1\\ u^3+v_2\end{pmatrix}.
	\end{equation*}
	We can explicitly construct an inverse 
	\begin{equation*}
		G^{-1}\begin{pmatrix}x\\ y\\ z\end{pmatrix} = H\begin{pmatrix}x\\ y\\ z\end{pmatrix} = \begin{pmatrix}x\\ y-x^2\\ z-x^3\end{pmatrix}.
	\end{equation*}
	Letting $F=H_2$, i.e. 
	\begin{align*}
		F:\mathbb{R}^3 \to& \mathbb{R}^2 \\
		\begin{pmatrix}x\\ y\\ z\end{pmatrix} \mapsto& \begin{pmatrix}y-x^2\\ z-x^3\end{pmatrix}
	\end{align*}
	and get $M$ as the zero-set of $F$.
\end{example}

\begin{definition}
	Let $M$ be a manifold as above. Its \emph{tangent space} at $p$, denoted $T_pM$, is defined in the following equivalent ways: 
	\begin{enumerate}
		\item If $M$ is locally the graph of $f$ with $p=(a, f(a))^T$, then $T_pM=\Gamma(Df(a))$.
		\item Assuming $M$ is locally a level set of $F$, then $T_pM=\mathbf{N}(DF(p))$.
		\item Assuming $M$ is locally parameterized by $g$ with $p=g(a)$, then $T_pM=\text{im}(Dg(a):\mathbb{R}^n\to\mathbb{R}^n)$.
	\end{enumerate}
\end{definition}

% manifolds, revisited }}}1

% solving nonlinear problems }}}1

\section{appendix} % {{{1 

Given a plane $n\cdot x=d$ and a point $x_0$, the vector $x_0-x$ is a vector from the plane to the point. To get the perpindicular distance, we project $x_0-x$ onto $n$:
\begin{equation*}
	\text{proj}_n(x_0-x)=\frac{n\cdot (x_0-x)}{n\cdot n}n = \frac{n\cdot x_0-d}{n\cdot n}n.
\end{equation*}
Taking signed magnitude, 
\begin{equation*}
	D = \left\| \frac{n\cdot x_0-d}{n\cdot n}n\right\| = \frac{n\cdot x_0-d}{n\cdot n}\|n\|=\frac{n\cdot x_0-d}{\|n\|}.
\end{equation*}

% appendix }}}1

\end{document}
